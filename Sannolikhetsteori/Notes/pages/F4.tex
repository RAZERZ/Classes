\section{Medelvärde}\par
\noindent Vi börjar med ett exempel, myntkastet såklart där $\Omega = \left\{H,T\right\}^N$ och $N$ är väldigt stort.\par
\noindent Vi definierar sannolikhetsmåttet på rummet som $P(\omega) = \dfrac{1}{2^n}$.\par
\noindent Vi har även de stokastiska variablerna som spottar ut vad vi får på det $i$:te kastet,\par
\noindent $X_i(\omega) = \begin{cases}1, \omega_i = H\\0, \omega_i = T\end{cases}$
\par\bigskip
\noindent Om vi gör $n$ myntkast och $n$ är stort, förväntar vi oss att ha 50\% $H$ och 50\%  $T$ eller alternativt formulerat ca $\dfrac{n}{2}$ krona. Detta är en frekvenstolkning.
\par\bigskip
\noindent Med andra ord, förväntar vi oss följande:
\begin{equation*}
  \begin{gathered}
    X_1+\cdots+X_n =\dfrac{n}{2} \text{ med stor sannolikhet}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Det här med "stor sannolikhet" är viktigt, eftersom man \textit{tekniskt sett} kan dra krona krona krona $\cdots$.
\par\bigskip
\begin{theo}[Stora talens lag]{thm:bignumhe}
  Om $X_1,X_2,X_3,\cdots$ är obereonde och likafördelade slumpvariabler så har vi, för varje $\varepsilon>0$:
  \begin{equation*}
    \begin{gathered}
      P\left(\left|\dfrac{x_1+\cdots+x_n}{n}-E(X_i)\right|>\varepsilon\right)\underbrace{\to}_{\text{$n\to\infty$}} 0\text{ för något tal $E(X_i)$}
    \end{gathered}
  \end{equation*}
  \par\bigskip
  \noindent För diskreta slumpvariabler är:
  \begin{equation*}
    \begin{gathered}
      E(X) = \sum_{x}xP_X(x)\text{ om summan är absolutkonvergent ($\exists$ vissa specialfall)}
    \end{gathered}
  \end{equation*}
  \par\bigskip
  \noindent Förutsatt att summan ej beror på ordningen av termer (absolutkonvergent eller $X\geq0$ eller $0\geq X$). En slags mittpunkt för sannoliketsrummet.
\end{theo}
\par\bigskip
\begin{theo}[Väntevärdet/Medelvärde]{thm:wogh}
  Talet $E(X)$ kallas \textit{väntevärdet}/\textit{medelvärdet} till $X$
\end{theo}
\par\bigskip
\noindent Tänk såhär, om $n$ är stort, förväntar vi oss cirka $n\cdot p$st $x$ om $P_X(x) = p$, så vi förväntar oss alltså $X_1+\cdots+X_n = \sum x\cdot nP_X(x)$ och $\dfrac{X_1+\cdots+X_n}{n}\to\sum xP_X(x)$\par
\noindent (Vi summerar över alla $X$ med positiv sannolikhet)
\par\bigskip
\noindent Vi kan definiera $E(X) = \sum xP_X(x)$ om summan är $\infty$ för varje ordning av termer (samma för $-\infty$), exempelvis om $X\geq0$
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent Säg att $X\sim Be(p)$, då är $P_X(1)=p$, $P_X(0)=1-p$, $E(X) = 1\cdot p + 0\cdot(1-p) = p$
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent $X\sim Hyp(N,n,m)$:
\begin{equation*}
  \begin{gathered}
    E(X) = \sum_{k=0}^{n}kP_X(k) = \sum_{k=0}^{n}k\dfrac{\begin{pmatrix}m\\k\end{pmatrix}\begin{pmatrix}N-m\\n-k\end{pmatrix}}{\begin{pmatrix}N\\n\end{pmatrix}}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Per definition har vi att $\begin{pmatrix}n\\k\end{pmatrix} = \dfrac{n!}{(n-k)!k!} = n\dfrac{(n-1)!}{k(n-k)!(k-1)!} = \dfrac{n}{k}\begin{pmatrix}n-1\\k-1\end{pmatrix}$. Då är $E(X)$:
\begin{equation*}
  \begin{gathered}
    = \sum_{k=1}^{n}k\dfrac{m}{k}\dfrac{n}{N}\dfrac{\begin{pmatrix}m-1\\k-1\end{pmatrix}\begin{pmatrix}N-m\\n-k\end{pmatrix}}{\begin{pmatrix}N-1\\n-1\end{pmatrix}} = m\dfrac{n}{N}\underbrace{\sum_{k=0}^{n-1}\underbrace{\dfrac{\begin{pmatrix}m-1\\k\end{pmatrix}\begin{pmatrix}N-m\\n-1-k\end{pmatrix}}{\begin{pmatrix}N-1\\n-1\end{pmatrix}}}_{\text{$Hyp(N-1,n-1,m-1)$}}
    }_{\text{=1}} = n\dfrac{m}{N}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent Från envariabelanalys vet vi att $\sum_{n=1}^{\infty}\dfrac{1}{n^2}$ konvergerar mot $c$ ($c = \dfrac{\pi^2}{6}$)\par
\noindent Sätt $P_X(n) = \dfrac{1}{cn^2}\quad \forall n\in\N_+$. Då är $E(X)$:
\begin{equation*}
  \begin{gathered}
    = \sum_{n=1}^{\infty}n\dfrac{1}{cn^2} = \sum_{n=1}^{\infty}\dfrac{1}{cn} = \infty
  \end{gathered}
\end{equation*}
\par\bigskip
\begin{theo}[Law of the unconcious statistician]{thm:lotus}
  Givet en funktion $g:\R^n\to\R$ Vi har $E(g(X)) = \sum g(x)P_X(x)$
\end{theo}
\par\bigskip
\begin{prf}[Law of the unconcious statistician]{prf:lajf}
  \begin{equation*}
    \begin{gathered}
      E(g(X)) = \sum_{y:g(X)=y} yP(g(X)=y) = \sum_{y:g(X)=y}\sum_{x:g(x)=y}P(X=x)\\
      =\sum_{y:g(X)=y}\sum_{x:g(x)=y}\underbrace{y}_{\text{=$g(x)$}}P(X=x)\\
      =\sum_{y:g(X)=y}\sum_{x:g(x)=y}g(x)P(X=x)\\
      = \sum_{x}g(x)P(X=x) = \sum_{x}g(x)P_X(x)
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\begin{theo}
  VVi säger att $X\in L^1(\Omega)$ om $\sum\left|x\right|P_X(x)<\infty$.\par
  \noindent Mer generellt skriver vi att $X\in L^p(\Omega)$ om $\underbrace{\sum\left|x\right|^pP_X(x)}_{\text{$E(\left|X\right|^p)$}}<\infty$\par
  \noindent Med andra ord, $X\in L^p$ om $E(\left|X\right|^p)<\infty$
  \par\bigskip
  \noindent $L^P(\Omega) = \left\{X:\Omega\to\R:E(\left|X\right|^P)<\infty, X\text{ är diskret}\right\}$\par
  \noindent $L^1$ = absolutkonvergent = ändligt väntevärde
\end{theo}
\par\bigskip
\begin{theo}[Väntevärdet är linjärt]{thm:woghwoeg}
  $E(aX+bY) = aE(X)+bE(Y)\quad\forall a,b\in\R\quad X,Y\in L^1$
  \par\bigskip
  \noindent Eftersom $L^1$ är ett vektorrum så är $E:L^1\to\R$
\end{theo}
\newpage
\begin{prf}
  VVi sätter $g(x,y) = ax+by$. Då blir $g(X,Y) = aX+bY$\par
  \begin{equation*}
    \begin{gathered}
      E(aX+bY) = E(g(X,Y)) = \sum_{x,y} g(x,y)P_{X,Y}(x,y) = \sum (ax+by)P_{X,Y}(x,y)\\
      = a\sum_x\sum_y xP_{X,Y}(x,y)+b\sum_y\sum_x yP_{X,Y}(x,y)\\
      = \sum_x x\underbrace{\sum_y P_{x,y}(x,y)}_{\text{=$P(X=x,y\in\R)=P_X(x)$}}+b\sum_y y\underbrace{\sum_x P_{x,y}(x,y)}_{\text{=$P_Y(y)$}}\\
      = a\sum_x xP_X(x)+b\sum_y yP_X(y) = aE(X)+bE(Y)
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent Tag miljön för myntkast. Då var $X = X_1+\cdots+X_n\sim Bin(n,\dfrac{1}{2})$
\par\bigskip
\noindent Mer generellt, om $X_1,\cdots, X_n\sim Be(p)$ är obereonde så är $X=X_1+\cdots+X_n\sim Bin(n,p)$\par
\noindent Eftersom väntevärdet var en linjär operator och väntevärdet för $Be(p)=p$, så kommer $E(X) = np$.
\par\bigskip
\noindent Så om $X\sim Bin(n,p)$ så är $E(X)=np$
\par\bigskip
\noindent\textbf{Anmärkning:}\par
\noindent Alla $X\sim Bin(n,p)$ kan inte skrivas $X=X_1+\cdots+X_n$ där $X_1,\cdots,X_n$ är Bernoulli-fördelade!
\par\bigskip
\noindent Men, vi vet att det finns $X_1,\cdots, X_n\sim Be(p)$ som är oberoende och $X_1+\cdots+X_n\sim Bin(n,p)$, och alla binomialfördelade variabler har samma väntevärde. Enligt definitionen av väntevärdet är det enbart sannolikhetsfunktionen som bestämmer vad väntevärdet är.
\par\bigskip
\begin{theo}
  OOm $X$ är obereonde och $Y$ är obereonde, så är $E(XY) = E(X)E(Y)$
\end{theo}
\par\bigskip
\begin{prf}
  VVi visar detta på liknande sätt som tidigare:
  \begin{equation*}
    \begin{gathered}
      g(x,y) = xy\quad E(XY)\sum_{x,y}xy\underbrace{P_{X,y}(x,y)}_{\text{(oberoende) $\Rightarrow P_X(x)P_Y(y)$}}\\
      =\sum_xxP_X(x)\underbrace{\sum_yyP_Y(y)}_{\text{$E(Y)$}}\\
      = E(X)E(Y)
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\begin{theo}[Varians]{thm:varians}
  \textit{Variansen} av $X$ definieras genom:
  \begin{equation*}
    \begin{gathered}
      Var(X) = E((X-E(X))^2),\quad X\in L^2
    \end{gathered}
  \end{equation*}
\end{theo}
\par\bigskip
\noindent\textbf{Intuition:} \par
\noindent $X-E(X)$ är skillnaden mellan vad vi observerar och vad medelvärdet är, så om sannolikhetsfördelningen är utspridd så kommer vi observera många grejer som avviker och ligger långt ifrån väntevärdet. Om sannolikheten är liten, borde skillnaden vara liten.\par
\noindent Om $X$ avviker från $E(X)$ mycket så är variansen $Var(X)$ stor, om $X$ ligger nära $E(X)$ så är $Var(X)$ litet.
\par\bigskip
\noindent Tänk på det som ett medelvärde på hur mycket medelvärdet avviker från väntevärdet (\textbf{RÄTTA OM FEL})
\par\bigskip
\noindent Var kommer kvadraten ifrån då? Då måste vi kolla på standardavvikelsen som för $X$ definieras genom:
\begin{equation*}
  \begin{gathered}
    D(X) = \sqrt{Var(X)}\quad X\in L^2
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Varför inte $D(X) = E(\left|X-E(X)\right|)$? Skillnaden mellan det vi observerar och medelvärdet? (detta är medelavvikelsen från medelvärdet). Har inte detta mer tydligt betydelse då?
\par\bigskip
\noindent Svaret på varför vi inte definierar det på det sättet är att det är svårare att räkna på, belopp är jobbiga att räkna med. Kvadrater är lättare att räkna på, oavsett hur vi defnierar det så kommer det vara ett mått på hur mycket variabeln avviker från väntevärdet.
\par\bigskip
\noindent Detta går givetvis att mäta på många sätt, men vår definition är lätt att räkna på.
\par\bigskip
\noindent Både $Var(X)$ och $D(X)$ är spridningsmått (hur mycket variabeln sprider sig på $\R$) och generellt är $Var(X)$ lättare att räkna på.
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent Låt $Y$ vara en slumpvariabel med fördelningsfunktionen $F_Y(t)=P(Y\leq t) = \begin{cases}0,t<0\\t^2, t\in[0,1]\\1, t>1\end{cases}$
\par\bigskip
\noindent Rita upp $F(t)$\par
\noindent Beräkna $P(Y\leq 0.5) = F_Y(0.5) = 0.5^2=0.25$\par
\noindent Beräkna $P(0.5<Y\leq 0.9)$. $\underbrace{P(Y\leq 0.9)}_{\text{0.81}} = P(\left\{Y\leq 0.5\right\}\cup\left\{0.5<Y<0.9\right\}) = \underbrace{P(Y\leq0.5)}_{\text{0.25}}+$\par\noindent$\Rightarrow 0.81-0.25=0.56$.\par
\noindent Eftersom de är disjunkta kan vi summera sannolikheterna.
\par\bigskip
\begin{theo}[Egenskaper hos fördelningsfunktioner]{thm:wigjgw}
  \begin{equation*}
    \begin{gathered}
      P(X<a)=\lim_{h\to0^+}F(a-h)
    \end{gathered}
  \end{equation*}
\end{theo}
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent Vid en produktion vill vi tillverka kolvar med en viss diameter. Vi har dock inte absolut precision, felet kan beskrivas med en slumpvariabel $Y$ = absolutfelet i diametern. Täthetsfunktionen till $Y$ är omvänt proportionell mot absolutfelet.
\par\bigskip
\noindent Bestäm täthetsfunktionen $f_Y(y)\quad y\in[1,5]$\par
\noindent $f_Y(y) = c\dfrac{1}{y}$. Vi måste även ha att integralen $\int_{-\infty}^{\infty}f_Y(y)dy=1$
\par\bigskip
\noindent Bestäm fördelningsfunktionen (primitiv funktion till täthetsfunktionen)\par
\noindent $P(Y\leq t) = \int_{-\infty}^{t}f_Y(y)dt$\par
\noindent Om $t\leq1\Rightarrow P(Y\leq t)=0$\par
\noindent Om $1\leq t\leq5\Rightarrow P(Y\leq t) = \int_{-\infty}^{t}f_Y(y)dy = \int_{1}^{t}f_Y(y)dy=\dfrac{\ln(t)}{\ln(5)}$
\newpage
\noindent\textbf{Exempel:}\par
\noindent Med tvåpunktsfördelning menas att $P_X(a)=p$ och $P_X(b)=1-p$ (notera att detta är $Be(p)$ om $a=1$ och $b=0$)\par
\noindent Beräkna $E(X)$ och $Var(X)$:\par
\noindent $E(X) = ap+b(1-p)$\par

\begin{equation*}
  \begin{gathered}
  Var(X) = E((X-E(X))^2) = E((X-(ap+b(1-p)))^2)\\
  =E(X^2+2XE(X)+(EX)^2)=E(X^2)\underbrace{-2(E(X)E(X))+(E(X))^2}_{\text{=$(E(X))^2$}}\\
  \Rightarrow E(X^2) = \sum x^2P_X(x) = a^2P_X(a)+b^2P_X(b) = a^2p+b^2(1-p)\\
  \Rightarrow Var(x) = a^2p+b^2(1-p) - (ap+b(1-p))^2 = p(1-p)(a-b)^2
  \end{gathered}
\end{equation*}
\par\bigskip
\subsection{Egenskaper för väntevärden}\hfill\\
\begin{itemize}
  \item Väntevärdet av en konstant slumpvariabel, är inget annat än en konstant
  \item $E(X^p)=\sum x^pP_X(x)$ (här sätter vi $g(x) = x^p$)
  \item $E(\left|X\right|) = \sum \left|X\right|P_X(x)$ (låt $g(x) = \left|x\right|$)
  \item $E(X)$ är ändlig $\Lrarr E\left|X\right|<\infty$
  \item $E(X+Y) = E(X)+E(Y)$ så länge väntevärderna är definierade (vi tillåter inte att ena är $\infty$ och den andra $-\infty$)
  \item $E(cX) = cE(X)\quad c\in\R$
  \item $\left|E(X)\right|\leq E(\left|(X)\right|)$ (Ye Olde' Triangelolikheten)
  \item $X\geq0\Rightarrow E(X)\geq0$
  \item $X\leq Y\Rightarrow E(X)\leq E(Y)$
\end{itemize}
\par\bigskip
\noindent\textbf{Proposition}:\par
\noindent Om $q>p$ så är $L^q\subseteq L^p$
\par\bigskip
\begin{prf}
  VVi skriver $1_A(\omega) = \begin{cases}1, \omega\in A\\0,\omega\notin A\end{cases}$\par\bigskip
  \noindent $\left|X\right|^P = \left|X\right|^P1_{X\leq1}+\left|X\right|^P1_{X>1}$:
  \begin{equation*}
    \begin{gathered}
      \Rightarrow E(\left|X\right|^P) = \underbrace{E\left(\underbrace{\underbrace{\left|X\right|^P}_{\text{$\leq1$}}1_{x\leq1}}_{\text{$\leq1$}}\right)}_{\text{$\leq1$}} + E(\underbrace{\left|X\right|^P}_{\text{$\left|X\right|^q$}}\underbrace{1_{x>1}}_{\text{$\leq1$}})
    \end{gathered}
  \end{equation*}\par
  \noindent Så:
  \begin{equation*}
    \begin{gathered}
      E(\left|X\right|^q)<\infty\Rightarrow E(\left|X\right|^P)\leq1+E(\left|X\right|^q)<\infty
    \end{gathered}
  \end{equation*}
\end{prf}
\newpage
\noindent\textbf{Proposition:}\par
\noindent Om $X,Y\in L^P\Rightarrow X+Y\in L^P$
\par\bigskip
\begin{prf}
  NNotera att $\left|X\right|\leq \max\left\{\left|X\right|,\left|Y\right|\right\}\leq\left|X\right|+\left|Y\right|$.\par
  \noindent Då gäller även följande (för $p\geq1$):
  \begin{equation*}
    \begin{gathered}
    \left|X+Y\right|^P\leq (\left|X\right|+\left|Y\right|)^P\leq \left(2\max\left\{\left|X\right|,\left|Y\right|\right\}\right)^P\leq = 2^P\max\left\{\left|X\right|,\left|Y\right|\right\}\leq 2^P\left(\left|X\right|^P+\left|Y\right|^P\right)\\
    \Rightarrow E(\left|X+Y\right|^P)\leq 2^P(E(\left|X\right|^P)+E(\left|Y\right|^P))<\infty
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\noindent\textbf{Proposition:}\par
\noindent Om $X,Y\in L^2$ så $XY\in L^1$
\par\bigskip
\begin{prf}
  UUppenbarligen gäller:
  \begin{equation*}
    \begin{gathered}
      \left|XY\right|\leq\dfrac{X^2+Y^2}{2}
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\noindent Variansen av $X$ defnierades som $E(X-E(X))^2$. Ett mått på hur mycket variabeln avviker från väntevärdet.\par
\noindent Standardavvikelsen definierade vi som $D(X) = \sqrt{Var(X)}$. En grej vi kan notera direkt är att $Var(X)$ alltid är positiv, alltså alltid defnierad.
\par\bigskip
\noindent\textbf{Proposition:}\par
\noindent $Var(X)<\infty\Lrarr X\in L^2$. För $X\in L^2$ har vi:
\begin{equation*}
  \begin{gathered}
    Var(X) = E(X^2)-(E(X))^2
  \end{gathered}
\end{equation*}
\par\bigskip
\begin{prf}
  DDetta följer från
  \begin{equation*}
    \begin{gathered}
      E(X-E(X))^2= E(X^2\underbrace{-2XE(X)+(EX)^2}_{\text{ändlig}}) = E(X^2)-\underbrace{-2E(X)E(X)}_{\text{$2(E(X))^2$}}+(E(X))^2\\
      \Rightarrow E(X^2)-(E(X))^2
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\noindent Vi säger att $X\&Y$ är \textit{okorrelerade} om $E(XY)=E(X)E(Y)$ för $X,Y\in L^2$\par
\noindent Notera, oberonde $\Rightarrow$ okorrelerade, men inte tvärtom!
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent $P_X(-1) = P_X(0)=P_X(1) = \dfrac{1}{3}$.\par
Då är $E(X) = -1*\dfrac{1}{3}+0*\dfrac{1}{3}+1*\dfrac{1}{3} = 0$\par
Då är $E(X^3)$ = $(-1)^3*\dfrac{1}{3}+0^3*\dfrac{1}{3}+1^3*\dfrac{1}{3} = 0$
\par\bigskip
\noindent Då är $X\& X^2$ okorrelerade, men $X$ och $X^2$ kan ju inte vara oberonde!\par
\noindent $P(X=0,X^2=0) = P(X=0) = \dfrac{1}{3}\neq P(X=0)P(X^2=0)=\dfrac{1}{3}*\dfrac{1}{3}$
\newpage
\noindent Varför bryr vi oss om okorrelerade variabler? Jo:
\par\bigskip
\noindent\textbf{Proposition:}\par
\noindent $Var(X+Y) = Var(X)+Var(Y)\Lrarr X,Y$ är okorrelerade ($X,Y\in L^2$)
\par\bigskip
\begin{prf}
  VVi betraktar $Var(X+Y)$ som var $E(X+Y)^2-\left(E(X+Y)\right)^2$. Detta blir:
  \begin{equation*}
    \begin{gathered}
      E(X^2+2XY+Y^2)-\left((E(X))^2+2E(X)E(Y)+(E(Y))^2\right)\\
      \underbrace{\left(E(X^2)-(E(X))^2\right)}_{\text{$Var(X)$}} + \underbrace{\left(2E(XY)-2E(X)E(Y)\right)}_{\text{$=0\Lrarr$ X\&Y okorr.}}+\underbrace{\left(E(Y^2)-(E(Y))^2\right)}_{\text{$Var(Y)$}}
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\noindent\textbf{Anmärkning:}\par
\noindent Vad är $Var(cX)$?:
\begin{equation*}
  \begin{gathered}
    Var(cX) = E(cX)^2-\left(E(cX)\right)^2\\
    E(c^2X^2)-(cE(X))^2 = c^2E(X^2)-c^2(E(X))^2= c^2Var(X)
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Proposition:}\par
\noindent Om $X_1$ och $Y$ är okorrelerade och $X_2$ och $Y$ är okorrelerade, så är $X_1+X_2$ och $Y$ okorrelerade.
\par\bigskip
\begin{prf}
  VVi har givet att $E(X_1Y) = E(X_1)E(Y)$ och $E(X_2Y)=E(X_2)E(Y)$.\par
  \noindent Vi vill kolla vad $E((X_1+X_2)Y)$:
  \begin{equation*}
    \begin{gathered}
      = E(X_1Y+X_2Y) = E(X_1Y)+E(X_2Y) = E(X_1)E(Y)+E(X_2)E(Y)\\
      \Rightarrow (E(X_1)+E(X_2))E(Y) = E(X_1+X_2)E(Y)
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\noindent\textbf{Proposition:}\par
\noindent Om $X_1,\cdots, X_n$ är parvis okorrelerade, så $Var(X_1+\cdots+X_n) = Var(X_1)+\cdots+Var(X_n)$\par
\noindent Viktigaste specialfallet är när de är oberoende (ty det implicerar okorrelerade och vi kan då separera summorna).
\par\bigskip
\begin{prf}
  VVi kommer ihåg $Var(X_1+X_2) = Var(X_1)+Var(X_2)$. Om de är parvis okorrelerade bör ju även $X_1+X_2$ och $X_3$ vara okorrelerade enligt Bevis 7.9. Men detta betyder att:
  \begin{equation*}
    \begin{gathered}
      Var(X_1+X_2+X_3) = Var(X_1+X_2)+Var(X_3)= Var(X_1)+Var(X_2)+Var(X_3)
    \end{gathered}
  \end{equation*}\par
  \noindent Fortsätt med induktion
\end{prf}
\par\bigskip
\begin{theo}[Markovs olikhet]{thm:markovineq}
  Om $X\in L^1$ (dvs $E(\left|X\right|)<\infty$) så är $P(\left|X\right|\geq a)\leq\dfrac{E(\left|X\right|)}{a}$ för $a>0$\par
  \noindent Ju större $a$ är, desto mindre borde mängden $P(\left|X\right|\geq a)$ vara.
\end{theo}
\newpage
\begin{prf}[Markovs olikhet]{prf:goewughw}
  \begin{equation*}
    \begin{gathered}
      \left|X\right| = \left|X\right|1>{X\geq a}+\left|X\right|1_{x<a}\\
      E(\left|X\right|) = E(\left|X\right|1_{X\geq a})+\underbrace{E(\underbrace{\left|X\right|1_{x<a}}_{\text{$\geq0$}})}_{\text{$\geq0$}}\geq E\left(\underbrace{\left|X\right|}_{\text{$\geq a$}}1_{X\geq a}\right)\\
      \leq E(a1_{X\geq a}) = aE(1_{X\geq a}) = a(1*P(X\geq a)+0*P(X<a)) = aP(X\geq a)
    \end{gathered}
  \end{equation*}\par
  \noindent Alltså $E(\left|X\right|)\geq aP(X\geq a)$
\end{prf}
\par\bigskip
\noindent En följd av detta är $X\in L^P\Rightarrow P(\left|X\right|\geq a)\leq\dfrac{E\left|X\right|^P}{a^P}$
\par\bigskip
\begin{prf}
  VVi ser:
  \begin{equation*}
    \begin{gathered}
      P(\left|X\right|\geq a) = P(\left|X\right|^P\geq a^P)\leq\dfrac{E\left|X\right|^P}{a^P}
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\begin{theo}[Chebyshevs olikhet]{thm:ahvcehwc}
  \begin{equation*}
    \begin{gathered}
      P(\left|X-E(X)\right|\geq\varepsilon)\leq \dfrac{Var(X)}{\varepsilon^2}\quad (\varepsilon>0, X\in L^2)
    \end{gathered}
  \end{equation*}
\end{theo}
\par\bigskip
\begin{prf}[Chebyshevs olikhet]{prf:ahvcehwc}
  Sätt $P=2$ i förra satsen:
  \begin{equation*}
    \begin{gathered}
      P(\left|X\right|\geq\varepsilon)\leq\dfrac{E\left|X\right|^2}{\varepsilon^2}
    \end{gathered}
  \end{equation*}\par
  \noindent Byt $\left|X\right|$ mot $\left|X-E(X)\right|$:
  \begin{equation*}
    \begin{gathered}
      P(\left|X-E(X)\right|\geq\varepsilon)\leq\dfrac{E(\left|X-E(X)\right|^2)}{\varepsilon^2} = \dfrac{Var(X)}{\varepsilon^2}
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\begin{theo}[Annorlunda Stora talens lag]{thm:biogihwgnumhe}
  Antag $X_1,X_2\cdots\in L^2$ (oändlig följd av okorrelerade slumpvariabler)\par
  \noindent Antag även att $E(X_1) = E(X_2)=\cdots = \mu\in\R$ och $Var(X_1)=Var(X_2)=\cdots=\sigma^2\in\R$\par
  \noindent Vi skriver $\overline{X_n}$ för medelvärdet:
  \begin{equation*}
    \begin{gathered}
      \overline{X_n} = \dfrac{X_1+\cdots+X_n}{n}
    \end{gathered}
  \end{equation*}\par
  \noindent För $\varepsilon >0$ har vi:
  \begin{equation*}
    \begin{gathered}
      \lim_{n\to\infty}P\left(\left|\overline{X_n}-\mu\right|\geq\varepsilon\right) = 0
    \end{gathered}
  \end{equation*}
\end{theo}
\newpage
\begin{prf}[Annorlunda Stora talens lag]{prf:wigbwrg}
  \begin{equation*}
    \begin{gathered}
      E(\overline{X_n}) = E\left(\dfrac{1}{n}(X_1+\cdots+X_n)\right) = \dfrac{E(X_1)+\cdots E(X_n)}{n} = \dfrac{n\mu}{n} = \mu\\
      Var(\overline{X_n}) = Var\left(\dfrac{1}{n}(X_1+\cdots+X_n)\right) = \dfrac{1}{n^2}Var(X_1+\cdots+X_n)\\
      \Rightarrow \dfrac{1}{n^2}(\overbrace{Var(X_1)}^{\text{$\sigma^2$}}+\cdots+\overbrace{Var(X_n)}^{\text{$\sigma^2$}}) = \dfrac{1}{n^2}\cdot n\sigma^2 = \dfrac{\sigma^2}{n}\\
      \text{Chebyshevs olikhet sade } P\left(\left|\overline{X_n}-\underbrace{E(\overline{X_n})}_{\text{$\mu$}}\right|\geq\varepsilon\right)\leq \dfrac{Var(\overline{X_n})}{\varepsilon^2} = \dfrac{\sigma^2}{n\varepsilon^2}\overbrace{\to}^{\text{$n\to\infty$}}=0
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\noindent Detta kallas oftast för "baby stora talens lag", det finns fler, men vi håller oss till denna i denna kurs.
\par\bigskip
\subsection{Kovarians}\hfill\\\par
\noindent \textit{Kovariansen} av den 2-dimensionella slumpvariabeln $(X,Y)\in L^2$ (väntevärderna av kvadraterna är ändliga) betecknas:
\begin{equation*}
  \begin{gathered}
    Cov(X,Y) = E\left((X-E(X))(Y-E(Y))\right)\\
    = E(XY-XE(Y)-YE(X)+E(X)E(Y))\\
    = E(XY)-E(X)E(Y)-E(X)E(Y)+E(X)E(Y)\\
    = E(XY)-E(X)E(Y) = Cov(X,Y)
  \end{gathered}
\end{equation*}\par
\noindent Ett slags spridningsmått/varians för det 2-dimensionella fallet.
\par\bigskip
\noindent\textbf{Egenskaper:}\par
\begin{itemize}
  \item $Cov(X,X) = Var(X)$
  \item $Cov(X,Y) = Cov(Y,X)$
  \item $Cov(aX,Y) = aCov(X,Y) = Cov(X,aY)$
  \item $Cov(X_1+X_2,Y) = Cov(X_1,Y)+Cov(X_2,Y)$
  \item $Cov(a,X) = 0 = Cov(X,a)$
  \item $Cov(X,Y) = 0\Lrarr E(XY) = E(X)E(Y)\Lrarr X,Y$ är okorrelerade
  \item $X,Y$ oberonde $\Rightarrow$ okorrelerade $\Rightarrow Cov(X,Y) = 0$
  \item $Var(X+Y) = Var(X)+Var(Y)+2Cov(X,Y)$
  \item $Var(X-Y) = Var(X)+Var(Y)-2Cov(X,Y)$
  \item $Var(X_1+\cdots+X_n) = Var(X_1)+\cdots+Var(X_n)+2\sum_{i<j} Cov(X_i,X_j)$
\end{itemize}\par
\noindent Notera! Detta betyder alltså att $Cov$ är en bilinjär funktion!\par
\noindent Notera! första 4 punkter påminner om en inre produkt i linjär algebra, men $Cov$ är inte en inre produkt på $L^2$
\par\bigskip
\noindent När är $Cov(X,X) = Var(X) = 0$?
\begin{equation*}
  \begin{gathered}
    Var(X) = E(X-E(X))^2 = \sum_{x}\underbrace{(x-E(X))^2P_X(x)}_{\text{positiva, alla termer =0}}= 0
  \end{gathered}
\end{equation*}\par
\noindent Om $x\neq E(X)$ så måste $P_X(x)=0\Rightarrow P_X(E(X))=1$. Alltså om $Var(X)=0$ så måste $X=E(X)$ med sannolikhet 1, med andra ord $X$ vara konstant på en mängd med sannolikhet 1. ($X$ är nästan konstant).
\par\bigskip
\noindent Definiera en ekvivalensrelation $\sim$ på $L^2$ genom $X\sim Y$ om $X-Y$ är konstant med sannolikhet 1 (nästan konstant). Det finns en delmängd med sannolikhet 1 och för den delmängden så spottar $X-Y$ en konstant.
\par\bigskip
\noindent Ekvivalensklasser: $[X] = \left\{Y:Y\sim X\right\}$\par
\noindent Vi kan definiera $[X]+[Y] = [X+Y]$ och $a[X] = [aX]$, samt $Cov([X],[Y]) = Cov(X,Y)$. Alla dessa är väldefinierade.
\par\bigskip
\noindent Vi skriver $L^2/\sim = \left\{[X]:X\in L^2\right\}$ (vektorrum)\par
\noindent Kom ihåg, $X,Y\in L^2\Rightarrow X+Y\in L^2$, samt $aX\in L^2\quad (a\in\R)$. Då är $L^2$ också ett vektorrum.
\par\bigskip
\noindent Kovarians är väldefinierat på $L^2/\sim$ och blir nu en inre produkt på $L^2/\sim$\par
\noindent Från detta följer Cauchy-Schwarz olikhet, dvs $\left|Cov([X],[Y])\right|\leq Cov([X],[X])Xov([Y],[Y]) = \sqrt{Var([X])Var([Y])}$\par
\noindent Notera, "ortogonala vektorer" ger att inre produtken är 0, vilket i vårat fall betyder att $Cov(X,Y)=0$ vilket händer om $X,Y$ är okorrelerade.
\par\bigskip
\begin{theo}[Korrelationskoefficienten]{thm:fowejfwe}
  \begin{equation*}
    \begin{gathered}
      \rho(X,Y) = \dfrac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}\\
      \left|\rho(X,Y)\right|\leq1\\
      \rho(X,Y) =0\Lrarr X,Y\text{ okorrelerade}
    \end{gathered}
  \end{equation*}
\end{theo}
\par\bigskip
\noindent När är $\left|\rho(X,Y)\right|=1$?
\begin{equation*}
  \begin{gathered}
    \rho([X],[Y])=1\Lrarr \left|Cov([X],[Y])\right|= \sqrt{Var([X])Var([Y])}
  \end{gathered}
\end{equation*}\par
\noindent Likhet gäller $\Lrarr$ vektorerna är linjärt beroende, dvs $[Y] = a[X]\quad (a\in\R)$ eller om $[X] =0$\par
\noindent Med anda ord är $\left|\rho(X,Y)\right| = 1\Lrarr X-aY = b$ på en mängd med sannolikhet 1 för några $a,b\in\R$.
\par\bigskip
\noindent Tänk på $\rho$ som något slags mått på hur beroende variablerna är.
\par\bigskip
\noindent Den betingade sannolikhetsfunktionen $P_{X|Y}(x|y)$ är defnierad av (givet att $P_Y(Y)>0$):
\begin{equation*}
  \begin{gathered}
    P_{X|Y}(x|y) = P(X=x|Y=y) =\dfrac{P(X=x,Y=y)}{P(Y=y)}  = \dfrac{P_{X,Y}(x,y)}{P_Y(y)}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Lagen om total sannolikhet för detta fall blir:
\begin{equation*}
  \begin{gathered}
    P(X=x) = \sum_{y}\underbrace{P(X=x|Y=y)P(Y=y)}_{\text{$P(X=x,Y=y)$}}\\
    P_X(x) = \sum_{y}P_{X|Y}(x|y)P_Y(y)
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Kom ihåg: För varje $y$ så att $P_Y(y)>0$ så är $P_{X|Y}(x|y)$ en sannolikhetsfunktion. Vi säger inte till vilken slumpvariabel, men exempelvis till slumpvariabeln\par $X|Y=y=X|_{\{Y=y\}}:\underbrace{\left\{Y=y\right\}}_{\text{har sannolikhetsmått $Q(A)=P(A|Y=y)$}}\to\R$
\par\bigskip
\noindent Väntevärdet $E(X|Y=y) = \sum xP_{X|Y}(x|y)$
\par\bigskip
\noindent Det betingade väntevärdet $E(X|Y)$ är slumpvariabeln $E(X|Y)(\omega)=E(X|Y=y)$ om $Y(\omega) = y$. Detta gäller $\forall \omega\in\Omega$
\par\bigskip
\begin{theo}
  O $E(E(X|Y)) = E(X)$
\end{theo}
\newpage
\begin{prf}
  VVi sätter $g(y) = E(X|Y=y)$. Då är $g(Y) = E(X|Y)$
  \begin{equation*}
    \begin{gathered}
      E(E(X|Y)) = E(g(Y)) = \sum_y g(y)P_Y(y) = \sum_y E(X|Y=y)P_Y(y)\sum_y\sum_xxP_{X|Y}(x|y)P_Y(y)\\
      =\sum_x\sum_yxP_{X|Y}(x|y)P_Y(y) =\sum_xx\underbrace{\sum_yP_{X|Y}(x|y)P_Y(y)}_{\text{$P_X(x)$}}\\
      =\sum xP_X(x)=E(X)
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\subsection{Mer om kontinuerliga sannoliketsrum}\hfill\\\par
\noindent Standardexemplet är att slumpa ett reellt tal mellan 0 och 1. Alla möjliga utfall är givetvis talen mellan 0 och 1, så vårat utfallsrum är $\Omega=[0,1]$\par
\noindent Detta intervall har längd 1. Säg att vi har $0\leq a\leq b\leq 1$, vad är då sannolikheten att $P(x\in [a,b]) = b-a$ (längden av intervallet), eftersom längden ger hur stor del av intervallet $[0,1]$ som utgörs av $[0,1]$
\par\bigskip
\noindent Samma gäller för öppna intervall, samt halvöppna intervall\par
\noindent Givetvis finns specialfallet $P(\left\{a\right\})=P([a,a])=a-a=0$
\par\bigskip
\noindent Från Kolmogorovs axiom samt att $\Q$ är en uppräknelig mängd följer det att $P(\Q\cap [0,1]) = P(\bigcup_{q\in\Q\cap[0,1]}[q,q]) = \sum_{q\in\Q\cap[0,1]}P([q,q])\sum0=0$
\par\bigskip
\noindent För ett irrationellt tal $P([0,1\backslash\Q]) = \underbrace{P([0,1])}_{\text{1-0=1}}-\underbrace{P[0,1\cap\Q]}_{\text{0}} = 1-0=1$
\par\bigskip
\subsubsection{Cantormängden}
\par\bigskip
\noindent Låt $C_1 = [0,\dfrac{1}{3}]\cup[\dfrac{2}{3},1]$\par
\noindent Då blir $P(C_1) = P([0,\dfrac{1}{3}])+P([\dfrac{2}{3},1]) = \dfrac{1}{3}+\dfrac{1}{3}=\dfrac{2}{3}$
\par\bigskip
\noindent Vi låter nu $C_2 = [0,\dfrac{1}{9}]\cup[\dfrac{2}{9},\dfrac{1}{3}]\cup[\dfrac{2}{3},\dfrac{7}{9}]\cup[\dfrac{8}{9},1]$\par
\noindent $P(C_2) = 2^2\cdot\dfrac{1}{3^2}$\par
\par\bigskip
\noindent Och så fortsätter vi att dela ner intervallen i tredjedelar$\cdots$\par
\noindent Notera att vi även dubblar antal intervall för varje indelning. Alltså blir $P(C_n) = 2^n\cdot\dfrac{1}{3^n} = \left(\dfrac{2}{3}\right)^n$
\par\bigskip
\noindent Cantormängden definieras som $\bigcap_{n=1}^{\infty} = C_n$\par
\noindent Frågan är om det finns något finns kvar i mängden, för vi delar in i mindre och mindre delar.\par
\noindent Exempelvis ligger $0\in C$ samt ändpunkterna på delintervallen
\par\bigskip
\noindent Mer generellt; tag $x\in[0,1]$, med decimalutveckling $x = 0.x_1x_2x_3\cdots$ i bas 3, dvs $x = \sum_{n=1}^{\infty} = \dfrac{x_n}{3^n}$\par
\noindent Om $x_i\in\left\{0,2\right\}$ så är $x\in\bigcap_{n=1}^{\infty} C_n$. Det följer att denna mängd är ouppräknelig
\par\bigskip
\noindent Man kan ställa sig frågan, vad är då $P(C)$?\par
\noindent $P(C_n) = \dfrac{2^n}{3^n}$. Vi vet att $C_n\subseteq\cdots C_3\subseteq C_2\subseteq C_1$\par
\noindent Om Kolmogorovs axiom håller måste vi har $P(C) = P\left(\bigcap_{n=1}^{\infty}C_n\right) = \lim_{n\to\infty}P(C_n) = \lim_{n\to\infty}\left(\dfrac{2}{3}\right)^n=0$
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent Om vi ändrar lite på exemplet, det vill säga slumpa tal mellan 0 och 2. Då är $\Omega = [0,2]$ där\par\noindent $P(A) = \dfrac{\text{längden av $A$}}{2}$ 
\par\bigskip
\begin{theo}[Teaser: Kontinuerligt likformig slumpvariabel]{thm:woghwntoirjh}
  En slumpvariabel $X:\Omega\to\R$ kallas \textit{kontinuerligt likformig kontinuerligt} om:
  \begin{equation*}
    \begin{gathered}
      \exists I=[a,b],a<b\\
      \text{Om } a\leq c\leq d\leq b: P\left(X\in[c,d]\right)=\dfrac{\overbrace{d-c}^{\text{$\left|[c,d]\right|$}}}{\underbrace{b-a}_{\text{$\left|[a,b]\right|$}}}
    \end{gathered}
  \end{equation*}
\end{theo}
\par\bigskip
\noindent Om slumpvariabeln $X$ är likformig fördelad på ett intervall $(a,b)$ säger vi $X\sim U(a,b)$
\begin{theo}[Absolut kontinuerlig fördelning]{thm:jfweoiv}
  En slumpvariabel $X:\Omega\to\R^n$ kallas \textit{absolut kontinuerlig} om det finns en Riemann integrerbar funktion $f:\R^n\to\R$ så att:
  \begin{equation*}
    \begin{gathered}
      P\left(X\in A\right) = \int_{A}f_X(x)dx
    \end{gathered}
  \end{equation*}\par
  \noindent För den endimensionella slumpvariabeln $X$ gäller följande:
  \begin{equation*}
    \begin{gathered}
      P(a\leq X\leq b) = \int_{a}^{b}f(x)dx
    \end{gathered}
  \end{equation*}\par
  \noindent Vi måste även ha:
  \begin{equation*}
    \begin{gathered}
      \int_{-\infty}^{\infty}f(x)dx=1
    \end{gathered}
  \end{equation*}\par
  \noindent för att Kolmogorovs andra axiom ska uppfyllas. \par
  \noindent En sådan funktion $f_X(x)$ kallas för en täthetsfunktion. \par
\end{theo}
\par\bigskip
\noindent Om $X\sim U(a,b)$, vad är då $f_X$?\par
\noindent Vi antar $a\leq c<d\leq b$
\begin{equation*}
  \begin{gathered}
    a\leq c\leq d \Rightarrow P\left(X\in[c,d]\right)=\dfrac{a-c}{b-a}=\int_{c}^{d}\dfrac{1}{b-a}dx\\
  P(X\in[a,b])=\dfrac{b-a}{b-a}=1=\int_{a}^{b}\dfrac{1}{b-a}dx\\
  \Lrarr f_X(x)=
    \begin{cases}
      \dfrac{1}{b-a}, a<x\leq b\\
      0,\text{ annars}
    \end{cases}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent En endimensionell slumpvariabel $X:\Omega\to\R$ kallas normalfördelad om:
\begin{equation*}
  \begin{gathered}
    P(a\leq X\leq b) = \int_{a}^{b}\dfrac{1}{\sigma\sqrt{2\pi}}e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}dx\qquad \mu\in\R,\sigma>0
  \end{gathered}
\end{equation*}\par
\noindent Vi skriver $X\sim N(\mu,\sigma^2)$
\newpage
\begin{theo}[Fördelningsfunktion]{thm:fordelning}
  Fördelningsfunktionen till en slumpvariabel $X$ är $F_X(x)=P(X\leq x)$\par
  \noindent För kontinuerliga funktioner är vi främst intresserade över ett intervall:
  \begin{equation*}
    \begin{gathered}
      P(a\leq X\leq b) = P(\left\{X\leq b\right\}\backslash\left\{X\leq a\right\}) = P(X\leq b)-P(X\leq a)\\
        = F_X(b)-F_X(a)
    \end{gathered}
  \end{equation*}\par
  \noindent Om $X$ är absolutkontinuerlig så är:
  \begin{equation*}
    \begin{gathered}
      P(a\leq X\leq b) = F_X(b)-F_X(a)
    \end{gathered}
  \end{equation*}
  \par\bigskip
  \noindent $X$ kallas \textit{kontinuerlig} om $P(X=x)=0\quad\forall x\in\R$\par
  \noindent Om $X$ är kontinuerlig så är $P(a\leq X\leq b) = P(a<XX<b) = F_X(b)-F_X(a)$
\end{theo}
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent Säg att vi har $X\sim N(0,1)$, vi får då:
\begin{equation*}
  \begin{gathered}
    f_X(x) = \dfrac{1}{\sqrt{2\pi}}e^{-\dfrac{x^2}{2}}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Detta kallas för \textit{standardiserad normalfördelning}\par
\noindent Vi skriver $\varphi$ för fördelningsfunktionen atill $N(0,1)$, med andra ord:
\begin{equation*}
  \begin{gathered}
    \varphi(x) = \dfrac{1}{\sqrt{2\pi}}e^{-\dfrac{x^2}{2}}\quad x\in\R
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Om $X\sim N(,\mu,\sigma^2)$, så är
\begin{equation*}
  \begin{gathered}
    P(a\leq X\leq b) = \int_{a}^{b}\dfrac{1}{\sigma\sqrt{2\pi}}e^{-\dfrac{(x-\mu)^2}{\sigma^2}}dx\\
    \int_{\dfrac{a-\mu}{\sigma}}^{\dfrac{b-\mu}{\sigma}}\dfrac{1}{\sqrt{2\pi}}e^{-\dfrac{y^2}{2}}dy\\
    a\leq X\leq b\Lrarr \dfrac{a-\mu}{\sigma}\leq \dfrac{X-\mu}{\sigma}\leq \dfrac{b-\mu}{\sigma}\\
    \text{Så } P(a\leq X\leq b) = P(\dfrac{a-\mu}{\sigma}\leq \dfrac{X-\mu}{\sigma}\leq \dfrac{b-\mu}{\sigma})\\
    = \varphi\left(\dfrac{b-\mu}{\sigma}\right)-\varphi\left(\dfrac{a-\mu}{\sigma}\right)
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Sätt $a^{\prime} = \dfrac{a-\mu}{\sigma}$, $b^{\prime} = \dfrac{b-\mu}{\sigma}$ får vi:
\begin{equation*}
  \begin{gathered}
    P(a^{\prime}\leq \dfrac{X-\mu}{\sigma}\leq b^{\prime}) = \int_{a^{\prime}}^{b^{\prime}}\dfrac{1}{\sqrt{2\pi}}e^{-\dfrac{y^2}{2}}dy
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Med andra ord är $X\sim N(\mu, \sigma^2)\Lrarr\dfrac{X-\mu}{\sigma}\sim N(0,1)$
\newpage
\begin{theo}[Centrala  gränsvärdessatsen]{thm:centralligma}
  Om vi tar en massa slumpvariabler $X_1,X_2,\cdots\in L^2$ (de är alla oberoende), har väntevärdet $\mu$ med varians $\sigma^2>0$ och är lika fördelade (kan vara Bernoulli fördelade, Hypergeometrisk fördelade etc).
  \par\bigskip
  \noindent Då är medelvärdet $\overline{X_n} =\dfrac{X_1+\cdots+X_n}{n}\approx N(\mu,\sigma^2)$ fördelad, i meningen att:
  \begin{equation*}
    \begin{gathered}
      P\left(a\leq \dfrac{\overline{X_n}-\mu}{\sigma\sqrt{n}}\leq b\right)\to^{n\to\infty}\varphi(b)-\varphi(a)\\
      = \int_{a}^{b}\dfrac{1}{\sqrt{2\pi}}e^{-\dfrac{y^2}{2}}dy\\
      \text{Kom ihåg: } E(\overline{X_n}) = E(\dfrac{1}{n}(X_1+\cdots+X_n)) = \dfrac{n\mu}{n} = \mu\\
      Var(\overline{X_n}) = Var(\dfrac{1}{n}(X_1+\cdots+X_n)) \dfrac{1}{2}\left(Var(X_1)+\cdots\right) = \dfrac{n\sigma^2}{n^2} = \dfrac{\sigma^2}{n}\\
      D(\overline{X_n}) = \dfrac{\sigma}{\sqrt{n}}
    \end{gathered}
  \end{equation*}
\end{theo}
\par\bigskip
\noindent\textbf{Övning:}\par
\noindent Visa att om $E(X) = \mu\in\R$, $Var(X) = \sigma^2>0$, så är:
\begin{equation*}
  \begin{gathered}
    E\left(\dfrac{X-\mu}{\sigma}\right) = 0\qquad Var\left(\dfrac{X-\mu}{\sigma}\right)=1
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Specialfall:}\par
\noindent Om $X_i\sim Be(\dfrac{1}{2})$ (de Moivre's sats)\par
\noindent Generaliserar vi det till $X_i\sim Be(p)$ (de Moivre's-Laplace sats)
\par\bigskip
\noindent Betrakta $X_1,X_2$ som är oberoende slumpvariabler med $Be(\dfrac{1}{2})$:
\begin{equation*}
  \begin{gathered}
    X = \sum_{n=1}^{\infty}\dfrac{2X_n}{3^n}= \text{ decimalutveckling i bas 3}
  \end{gathered}
\end{equation*}\par
\noindent $Be$ är antingen 0 eller 1, $2X_n\in\left\{0,2\right\}$ med båda sannolikheter $\dfrac{1}{2}$ att anta.\par
\noindent Då gäller $X\in C$ (Cantormängden)
\par\bigskip
\noindent\textbf{Övning:}\par
\noindent Visa att $X$ ej är diskret, ej absolutkontinuerlig, men kontinuerlig.
