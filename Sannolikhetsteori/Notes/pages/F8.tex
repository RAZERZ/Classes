\section{Kort introduktion till måtteori}\par
\noindent Vi hade följande betecknkng $X\sim U(0,1)$ för att indikera att $X$  är likformigt fördelat, med andra ord så har fördelningen sannolikhetsmått $P((a,b)) = b-a$ (om $(a,b)\subseteq [0,1]$) $= P([a,b]) = P([a,b))$.\par
\noindent Vi visade även att $P(\Q)=0$ och att $P(C)=0$ (där $C$ är Cantormängden)
\par\bigskip
\noindent Men detta var bara några exempel, vad händer om vi har en godtycklig delmängd?\par
\noindent Dvs, vad är $P(A)$ för godtyckligt $A\subseteq[0,1]$?
\par\bigskip
\noindent\textbf{Förslag:} Om det är likformigt fördelning kan vi tänka oss att det ska vara längden av $A$ (hur stor del av $A$ täcker intervallet?). Vi kan kalla det för $P^*(A) = inf\left\{\sum_{n=1}^{\infty}(b_n-a_n)\text{ om } A\subseteq\bigcup(a_n,b_n)\right\}$\par
\noindent Detta kallas för det yttre måttet. Tyvärr kommer detta inte funka, utan vi kommer ha följande problem, $P^*:2^{[0,1]}\to\R$ är inte ett sannolikhetsmått (uppfyller ej Kolmogorovs axiom) enligt Banach-Tarski problemet.
\par\bigskip
\noindent Vad vi inser är att vi bnorde sluta försöka hitta en delmängd. Vi skrotar de fula delmängderna och behåller den fina.
\par\bigskip
\begin{theo}
 EEtt mått på en mängd $\Omega$ är en funktion $\mu:2^{\Omega}\to[0,\infty]$ som uppfyller:
  \begin{equation*}
    \begin{gathered}
      \mu\left(\bigcup_{n=1}^{\infty}A_n\right) = \underbrace{\sum_{n=1}^{\infty}\mu(A_n)}_{\text{$\infty$ om en term är $\infty$}}\qquad A_i\cap A_j=\O \quad (i\neq j)
    \end{gathered}
  \end{equation*}
  \par\bigskip
  \noindent Notera att $\mu$ är ett sannolikhetsmått om $\mu(\Omega) = 1$
\end{theo}
\par\bigskip
\noindent Vi vill definiera längden av delmängder, säg $A\subseteq\R$ (eller mer generellt, volym av $A\subseteq\R^n$)
\par\bigskip
\noindent Vi skriver det yttre måttet:
\begin{equation*}
  \begin{gathered}
    \mu^*(A) = inf\left\{\sum_{n=1}^{\infty}(b_n-a_n)\text{ om } \subseteq\bigcup(a_n,b_n)\right\}\quad A\subseteq\R
  \end{gathered}
\end{equation*}\par
\noindent För $A\subseteq\R^n$:
\begin{equation*}
  \begin{gathered}
    \mu^*(A) = inf\left\{ \sum_{n=1}^{\infty}(b_{k,1}-a_{k,1})\cdots(b_{k,n}-a_{k,n})\text{ om }A\subset \bigcup(a_{k,1},b_{k,1})\cdots (a_{k,n},b_{k,n})\right\}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Problemet kvarstår, yttre måttet är inte ett mått och det finns inget mått, säg $\mu:2^{\R}\to[0,\infty]$ så att om vi vill mäta intervall och vi vill att det ska vara $b-a$ och vi vill dessutom att måttet av $A+x$ ska vara samma $A$.\par
\noindent Det vill säga, vi vill att följande ska var uppfyllda:
\begin{equation*}
  \begin{gathered}
    (1)\quad\mu([a,b]) = b-a\\
    (2)\quad\mu(A+x) = \mu(A)
  \end{gathered}
\end{equation*}\par
\noindent $\mu^*$ uppfyller detta, men de uppfyller inte kraven från Definition 10.1 
\par\bigskip
\noindent Iden är följande:\par
\noindent Hitta en stor nog samling delmängder $F\subseteq 2^{\R^n}$ så att $\mu:F\to[0,\infty]$ uppfyller Definition 10.1\par
\noindent $F$ kommer vara följande:
\begin{equation*}
  \begin{gathered}
    B(\R^n) = \text{ minsta samling delmängder} F\subseteq 2^\Omega\text{ som uppfyller}:\\
    (1)\quad (a_1,b_1)x\cdots x(a_n,b_n)\in F\qquad -\infty\leq a_i\leq b_i\leq\infty\\
    (2)\quad A\in F\Rightarrow A^c\in F\\
    (3)\quad A_1,A_2,\cdots\in F\Rightarrow\bigcup_{n=1}^{\infty}A_n\in F
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Att det ens existerar ett "minsta" i en mängd är inte alltid garanterat, men i detta fall gäller det (vi kikar på detta lite senare).\par
\noindent Om vi har (1)-(3), så kommer vi även ha exempelvis:
\begin{equation*}
  \begin{gathered}
    \begin{cases}
      (4)\quad A_1,A_2,\cdots,\in F\Rightarrow\bigcap_{n=1}^{\infty}\left(\bigcup_{n=1}^{\infty}A_n^c\right)^c\in F\\
      (5)\quad B\backslash A = B\cap A^c \in F \text{ om } A,B\in F\\
      \vdots
    \end{cases}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Vi vill mäta allt som vi kan bilda om vi tar union, komplement, snitt, differens osv.\par
\noindent Tar vi en mängd $A\in B(\R^n)$, så kallas de för \textit{Borelmängder}
\par\bigskip
\begin{theo}[Specialfall av Caratheodorys sats]{thm:cartarahtwo}
  \begin{itemize}
    \item $\mu^*:B(\R^n)\to[0,\infty]$ uppfyller Definition 10.1 (Lebesgue)\par
      \noindent Dessutom är $\mu^*$ den enda funktionen som tar Borelmängder och spottar ut någonting mellan $0$ till $\infty$ så att $\mu^*((a,b)) = b-a$ (samma princip för $n\geq1$)
      \par\bigskip
      \noindent Man kan fråga sig hur stor är den här samlingen delmängder? Den är stor. Den är väldigt stor. 
    \item (Caratheodory) Låt $R$ vara mängden av delmängder på formen:
      \begin{equation*}
        \begin{gathered}
          I_1\cup\cdots\cup I_m\qquad\text{ för öppna intervall }\subseteq\R^n
        \end{gathered}
      \end{equation*}\par
      \noindent (Unionen av rektanglar)\par
      \noindent Om $\mu_0:R\to[0,\infty]$ mäter delmängderna och uppfyller kraven för ett mått (Definition 10.1), så finns en unik $\mu:B(\R^n)\to[0,\infty]$ så att $\mu(A) = \mu_0(A)$ för $A\in R$ 
      \par\bigskip
      \noindent Med andra ord, unionen av rektanglar bestämmer unikt vad måttet av allt annat är, vilket kanske inte är så förvånande om man tänker hur man approximerar exempelvis area (finare och finare rektanglar täcker arean). Detta påminner kanske lite om Riemann-integralen. 
  \end{itemize}
\end{theo}
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent Om vi sätter $\mu_0((a,b)) = b-a$ får vi precis vad första punkten i Sats 10.2 säger.\par
\noindent Mer generellt, om vi sätter
\begin{equation*}
  \begin{gathered}
    \mu_0((a,b)) = \int_{a}^{b}f(x)dx
  \end{gathered}
\end{equation*}\par
\noindent där $f\geq0$ och Riemann-integrerbar, kommer också uppfylla Definition 10.1
\par\bigskip
\noindent Om $\int_{-\infty}^{\infty}f(x)dx = 1$ får vi ett sannolikhetsmått. Vi har exempelvis kollat på likformig fördelning:
\begin{equation*}
  \begin{gathered}
    f(x) = 
    \begin{cases}
      \dfrac{1}{b-a}, x\in[a,b]\\
      0,x\notin[a,b]
    \end{cases}
  \end{gathered}
\end{equation*}\par
\noindent Eller exempelvis:
\begin{equation*}
  \begin{gathered}
    f(x)=\dfrac{1}{\sigma\sqrt{2\pi}}e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Sats 10.2 kommer bevisas i reell analys/integrationsteori. Detta är en maffig/enorm sats.
\newpage
\noindent Vi ska nu definiera om Sannolikhetsrum så att vi inte mäter alla delmängder, utan vissa:
\par\bigskip
\begin{theo}[Sannolikhetsrum V2]{thm:sannoolikhet}
  Tag en delmängd $F\subseteq 2^\Omega$, denna kallas för en $\sigma$-algebra om:
  \begin{itemize}
    \item $\O\in F$
    \item  $A\in F\Rightarrow A^c\in F$
    \item $A_1,A_2,\cdots, \in F\Rightarrow \bigcup_{n=1}^{\infty}A_n\in F$
  \end{itemize}
  \par\bigskip
  \noindent Ett sannolikhetsrum är trippeln $(\Omega, F, P)$ om $P:F\to[0,1]$ är ett sannolikhetsmått
\end{theo}
\par\bigskip
\noindent Om vi då har en funktion $\mu$ som inte är definierad på hela sannolikhetsrummet utan på $F$ så är det ett mått om måttet av unionen av $A_n$ är summan av måtten:
\begin{equation*}
  \begin{gathered}
    \mu:F\to[0,\infty]\qquad m\left(\bigcup_{n=1}^{\infty}A_n\right) = \sum_{n=1}^{\infty}m(A_n)\qquad A_i\cap A_j = \O\quad i\neq j
  \end{gathered}
\end{equation*}\par
\noindent Om $\mu(\Omega) = 1$ kallas $\mu$ ett \textit{sannolikhetsmått}
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent Exempel på $\sigma$-algebra är $B(\R^n)$ eller $2^\Omega$
\par\bigskip
\noindent Det som är fiffigt är att vi kan specifiera vad sannolikheten till ett intervall är, och sedan vet vi att vi har ett sannolikhetsmått.
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent $(\R,B(\R),P)$ med $P((a,b)) = \int_{a}^{b}\dfrac{1}{\sqrt{2\pi}}e^{-x^2/2}dx$ ($N(0,1)$-normalfördelning) är ett sannolikhetsrum.\par
\noindent Då kommer Caratheodorys sats ge ett sannolikhetsmått på Borelmängderna ($A$):
\begin{equation*}
  \begin{gathered}
    B(\R^n): P(A) = \int_{A}\dfrac{1}{\sqrt{2\pi}}e^{-x^2/2}
  \end{gathered}
\end{equation*}\par
\noindent Detta är bara ett skrivsätt, vi kan exempelvis integrera över $\Q$ och få 0 (enligt samma argument som tidigare där vi visade att $P(\Q) = 0$)
\newpage
\noindent En slumpvariabel kommer vi också definiera om:
\par\bigskip
\begin{theo}[Slumpvariabel V2]{thm:woihgqofwogwe}
  En slumpvariabel är en funktion $X:\Omega\to\R^n$ så att:
  \begin{equation*}
    \begin{gathered}
    \left\{X\left\{\in A\right\}\right\} = \left\{\omega\in\Omega|X(\omega)\in A\right\}\in F\quad \forall A\in B(\R^n)
    \end{gathered}
  \end{equation*}\par
  \noindent Framförallt om vi kan ge en sannolikhet till $\left\{a\leq X\leq b\right\}\in F$
  \par\bigskip
  \noindent Kontinuerlig om $\forall x\in\R\quad P(X=x)=0$ (saknar atomer)\par
  \noindent $X$ kallas absolutkontinuerlig om det finns en funktion $f_X$ så att $P(a\leq X\leq b) = \int_{a}^{b}f_X(x)dx\Lrarr F_X(t) = \underbrace{\int_{-\infty}^{t}f_X(x)dx}_{\text{täthetsfunktion}}$
  \par\bigskip
  \noindent En absolutkontinuerlig slumpvariabel bestäms unikt av $f_X$
\end{theo}
\par\bigskip
\begin{theo}[Fördelning]{thm:fordelningwf}
  $P(X\in A)$ är ett mått $Q:B(\R^n)\to[0,1]$
  \par\bigskip
  \noindent Fördelningen bestäms \textit{unikt} av $P\overbrace{(a\leq X\leq b)}^{\text{$a<b\in\R$}}$
  \par\bigskip
  \noindent Det enda som krävs är att veta $P(a\leq X\leq b)$, kvittar Borelmängderna.\par
  \noindent Den bestäms även unikt av $P(\underbrace{X}_{\text{fördeln. funk.}}\leq a)$
  \par\bigskip
  \noindent Fördelningsfunktioner $F_X$ har följande egenskaper:\par
  \begin{itemize}
    \item Om $s\leq t\Rightarrow F_X(s)\leq F_X(t)$
    \item $\lim_{h\to0^+}F_X(t+h) = F_X(t)$
    \item $\lim_{h\to0^-}F_X(t+h) = P(X<t)$ (Motsäger sida 25?)
    \item $\lim_{t\to\infty}F_X(t) = 1$
    \item $\lim_{t\to-\infty}F_X(t)=0$
  \end{itemize}\par
  \noindent Notera att $F_X$ är kontinuerlig i $t$ m $P(X=1)=0$\par
  \noindent Framförallt är $F_X$ en kontinuerlig funktion $\Lrarr X$ är kontinuerlig slumpvariabel\par
  \noindent Framförallt om $F_X$ är deriverbar så är $f_X = F_X^{\prime}$, så är $f_X$ en täthetsfunktion (gäller även om $F_X$ är deriverbar föutom i ändligt många punkter)
  \noindent $X$ är absolutkontinuerlig $\Lrarr $ det finns en funktion $f_X\geq0$ så att $\int_{a}^{b}f_X(x)dx = F_X(b)-F_X(a) = P(a\leq X\leq b)$
\end{theo}
\par\bigskip
\noindent\textbf{DIY Sammanfattning:}\par
\noindent Vi kan inte alltid ge en sannolikhet till varje delmängd, så vi gör det bara i en $\sigma$-algebra ($F\subseteq2^\Omega$)\par
\noindent På reella talen använder vi Borelmängder, och, det här är det viktigaste, bestäms sannolikhetsmåttet $P:F\to[0,1]$ unikt av $P((a,b))$ för $(a<b\in\R)$. Det bestäms också unikt av fördelningsfunktionen som var, säg $F(x) = P((-\infty,x])$ (exempelvis $P((a,b)) = \int_{a}^{b}\dfrac{1}{\sigma\sqrt{2\pi}}e^-{\dfrac{(x-\mu)^2}{2\sigma^2}}$, $\sigma>0, \mu\in\R$)
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent Likformig fördelning på ett slutet intervall $[a,b]$. Då brukar vi skriva en sådan slumpvariabel som $X\sim U(a,b)$\par
\noindent Täthetsfunktionen har vi kikat på tidigare och blev:
\begin{equation*}
  \begin{gathered}
    f_X(x) =\begin{cases}\dfrac{1}{b-a},\; x\in[a,b]\\0,\;x\notin[a,b]\end{cases}
  \end{gathered}
\end{equation*}\par
\noindent Fördelningsfunktionen har vi också räknat på tidigare:
\begin{equation*}
  \begin{gathered}
    F_X(t) = \begin{cases}0,\;x\leq a\\\dfrac{x-a}{b-a},\;x\in[a,b]\\1,\;x\geq b\end{cases}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent (Exponentialfördelning) $X\sim Exp(\beta)\quad (\beta>0)$, vi skriver:
\begin{equation*}
  \begin{gathered}
    f_X(x) = \begin{cases}\beta e^{-\beta x},\; x>0\\0,\;x<0\end{cases}
  \end{gathered}
\end{equation*}\par
\noindent Integralen behöver vara lika med 1 för att Kolmogorovs axiom skall gälla. Detta är upp till läsaren att verifiera. Vi finner fördelningsfunktionen:
\begin{equation*}
  \begin{gathered}
    F_X(t) = \int_{-\infty}^{t}f_X(x)dx\begin{cases}0,\; t\leq 0\\\int_{0}^{t}\beta e^{-\beta x}dx = -e^{-\beta x}|_0^t = 1-e^{-\beta t}\end{cases}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Anmärkning:}\par
\noindent Stor lutning i normalfördelning ger stor varians, dvs liten förflyttning ger stor varians
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent $X\sim N(\mu, \sigma^2)$ hade täthetsfunktion $f_X(x) = \dfrac{1}{\sigma\sqrt{2\pi}}e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}$
\par\bigskip
\begin{theo}[Överkurs]{thm:overcourse}
  Om $F$ uppfyller egenskaperna 1,2,3 under Sats 10.5, så är $F$ en fördelningsfunktion till någon slumpvariabel.
  \par\bigskip
  \noindent Mer generellt, om $F_1,F_2,\cdots$ uppfyller 1,2,3 så finns oberoende slumpvariabler $X_1,X_2,\cdots$ så att $F_k = F_{X_k}$
\end{theo}
\newpage
\noindent\textbf{Egenskaper:} Absolutkontinuerliga slumpvariabler $X,Y:\Omega\to\R$\par
\begin{itemize}
  \item $X,Y$ oberoende $\Lrarr f_{X,Y}(x,y)= f_X(x)f_Y(y)$ 
  \item Väntevärdet $E(X)$ definieras av:
    \begin{equation*}
      \begin{gathered}
        E(X) = \int_{-\infty}^{\infty}xf_X(x)dx\quad\text{om integralen absolutkonvergent eller om den är $\infty$ eller $-\infty$}
      \end{gathered}
    \end{equation*}
  \item $E(g(X)) = \int g(x)f_X(x)dx$ om integralen är definierad
  \item $X\in L^p\Lrarr E(\left|X\right|^p) = \int\left|X\right|^pf_X(x)dx<\infty$
  \item $Var(X) = E(X-E(X))^2 = \int(x-E(X))^2f_X(dx) = E(X)^2-(E(X))^2$ om $X\in L^2$
  \item $Cov(X,Y) = E(X-E(X))(Y-E(Y)) = E(XY)-E(X)E(Y)$ om $X,Y\in L^2$
  \item $E(g(X,Y)) = \int\int g(x,y)f_{X,Y}(x)dxdy$ om $(X,Y)$ är Absolutkontinuerliga
  \item $E(aX+bY) = aE(X)+bE(Y)$, $Var(cX) = c^2Var(X)$, $Var(X+Y) = Var(X)+Var(Y)$ om $\underbrace{E(XY) = E(X)E(Y)}_{\text{okorrelerade slumpvariabler}}$, kovariansen är bilinjär osv gäller även för Absolutkontinuerliga slumpvariabler
  \item $f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y)dy$ 
  \item Markovs olikhet är bevarad: $P(X\geq a)\leq \dfrac{E(X)}{a}$ om $a>0$
  \item Stora talens lag: $P(\left|\bar{X_n}-\mu\right|>\varepsilon)\stackrel{n\to\infty}{\to}\quad\forall\varepsilon>0$
  \item Faltningsformlerna: Om $X,Y$ är oberoende så:
    \begin{equation*}
      \begin{gathered}
        f_{X+Y}(z) = \int_{-\infty}^{\infty}f_X(x)f_Y(z-x)dx
      \end{gathered}
    \end{equation*}
  \item Betingad fördelning: $f_{X|Y}(x|y) = \dfrac{f_{X,Y}(x,y)}{f_(y)}$ om $f_Y(y)>0\;\forall y$
  \item Betingat väntevärde: $E(X|Y = y) = \int xf_{X|Y}(x|y)dx$
  \item $E(E(X|Y)) = E(X)$
\end{itemize}
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent Tag en exponentialfördelad variabel $X\sim Exp(\beta)$, vad är $E(X)$?
\begin{equation*}
  \begin{gathered}
    E(X) = \int_{-\infty}^{\infty}xf_X(x)dx = \int_{0}^{\infty}x\beta e^{-\beta x}dx = \dfrac{1}{\beta}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Övning:}\par
\noindent Visa att $Var(X) = \dfrac{1}{\beta^2}$
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent Tag en likformigt fördelad variabel och räkna $E(X)$\par
\noindent Likformig fördelad var om $f_X(x) = \begin{cases}\dfrac{1}{b-a},\; x\in[a,b]\\0,\;x\notin[a,b]\end{cases}$:
\begin{equation*}
  \begin{gathered}
    E(X) = \int_{-\infty}^{\infty}xf_X(x)dx = \int_{a}^{b}\dfrac{x}{b-a}dx = \dfrac{x^2}{2(b-a)}|_a^b = \dfrac{b^2-a^2}{2(b-a)} = \dfrac{b+a}{2}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Vi passar på att räkna ut $Var(x):$
\begin{equation*}
  \begin{gathered}
    E(X)^2 = \int_{a}^{b}x^2f_X(x)dx = \dfrac{x^2}{3(b-a)}|_a^b = \dfrac{b^3-a^3}{3(b-a)}\\
    (E(X))^2 = \left(\dfrac{b+a}{2}\right)^2\\
    Var(X) = \dfrac{b^3-a^3}{3(b-a)} - \left(\dfrac{b+a}{2}\right)^2 = \dfrac{4(b^2+ab+a^2)-3(b^2+2ab+a^2)}{12} = \dfrac{b^2-2ab+a^2}{12}\\
    =\dfrac{(b-a)^2}{12}\quad\Leftarrow\text{Beror bara på $b-a$, variansen bryr sig inte om vi förflyttar intervallet}
  \end{gathered}
\end{equation*}
\newpage
\noindent\textbf{Övning:}\par
\noindent Om $X\sim N(\mu, \sigma^2)$ så är $E(X) = \mu$ och $Var(X) = \sigma^2$.\par
\noindent Tips: Utnyttja att $\int_{-\infty}^{\infty}e^{-x^2}dx = \sqrt{\pi}$\par
\noindent Använd Faltningsformlerna för att visa att om $X\sim N(0,1), Y\sim N(0,1)$ och $X,Y$ obereonde, så är $X+Y\sim N(0,2)$ fördelad\par
\noindent Mer generellt, om $X\sim N(\mu_1,\sigma_1^2)$ och $Y\sim N(\mu_2,\sigma_2^2)$ ($X,Y$ oberoende) så är $X+Y\sim N(\mu_1+\mu_2, \sigma_1^2+\sigma_2^2)$
\par\bigskip
\noindent Om $X\sim Exp(\lambda)$ och $Y\sim Exp(\lambda)$ ($\lambda>0$) och $X,Y$ obereonde, vad är täthetsfunktionen till $X+Y$?\par
\noindent Vi använder faltningsformlerna:
\begin{equation*}
  \begin{gathered}
    f_{X+Y}(z) = \int_{-\infty}^{\infty}\overbrace{f_X(x)}^{\text{$>0\Lrarr x>0$}}\underbrace{f_Y(z-x)}_{\text{$x<z$}}dx\\
    X\sim Exp(\lambda)\Lrarr F_X(t) = \begin{cases}1-e^{-\lambda t},\; t>0\\0,\;t\leq0\end{cases}\\
    f_X(x) = \begin{cases}\lambda e^{-\lambda x},\;t>0\\0,\;t\leq0\end{cases}\quad E(X) = \dfrac{1}{\lambda}\qquad Var(X) = \dfrac{1}{X^2}\\
    \Rightarrow f_{X+Y} = \begin{cases}0,\; z\geq0\\\int_{0}^{z}\lambda e^{-\lambda x}\lambda e^{-\lambda(z-x)}dx,\;z>0\end{cases}\\
    =\lambda^2\int_{0}^{z}e^{-\lambda x}e^{-\lambda z+\lambda x}dx = \lambda^2\int_{0}^{z}e^{-\lambda x}e^{-\lambda z}e^{\lambda x}dx\\
     = \lambda^2 ze^{-\lambda z}\\
     \Rightarrow f_{X+Y}(z) = \begin{cases}\lambda^2 ze^{-\lambda z},\;z>0\\0,\;z\leq0\end{cases}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent $X+Y$ är alltså $\Gamma(2,\lambda)$
\par\bigskip
\noindent\textbf{Övning:}\par
\noindent Visa att om $X_1,\cdots,X_n\sim Exp(\lambda)$ är obereonde så är $X_1+\cdots+X_n\sim\Gamma(n,\lambda)$\par
\noindent Detta går ut på samma sätt, visa med induktion
\par\bigskip
\noindent\textbf{Övning:}\par
\noindent Om $X\sim Exp(\lambda_1)$ och $Y\sim Exp(\lambda_2)$ och $\lambda_1\neq\lambda_2$ och $X,Y$ obereonde, vad är då täthetsfunktionen $f_{X+Y}$?
\par\bigskip
\noindent\textbf{Övning:}\par
\noindent Om $X\sim Exp(\lambda_1)$ och $Y\sim Exp(\lambda_2)$ och $X,Y$ obereonde, vad är då fördelningen till $Z=\min\left\{X,Y\right\}$?\par
\noindent Vad som menas är $Z(\omega) = \min\left\{X(\omega), Y(\omega)\right\}$ (minsta av de)
\par\bigskip
\noindent\textbf{Lösning:}\par
\noindent Tricker är att kolla på fördelningsfunktionen. Vad är sannolikheten att $P(Z>z) = P(\min\left\{X,Y\right\}>z) = 1-F_Z(z)$:
\begin{equation*}
  \begin{gathered}
    \min\left\{X,Y\right\}>z\Lrarr X>z\quad\& Y>z\\
    P(Z>z) = P(X>z,\:Y>z)\stackrel{ober.}{=}P(X>z)P(Y>z)\\
    =(1-F_X(z))(1-F_Y(z)) = \begin{cases}1,\;z\leq0\\e^{-\lambda_1z}e^{-\lambda_2z},\;z>0\end{cases}\\
    = \begin{cases}e^{-(\lambda_1+\lambda_2)z},\;z>0\\1,\;z\leq0\end{cases}\\
    \Rightarrow F_Z(t) = \begin{cases}1-e^{-(\lambda_1+\lambda_2)t},\;t>0\\0,\;t\leq0\end{cases}\\
  \Rightarrow\min\left\{X,Y\right\}\sim Exp(\lambda_1+\lambda_2)
  \end{gathered}
\end{equation*}
\newpage
\noindent\textbf{Övning:}\par
\noindent  Om $X\leq0$ är $Exp(\lambda_2)$-fördelad $(\lambda>0)$, vad är täthetsfunktionen till $\sqrt{X}$?
\par\bigskip
\noindent\textbf{Lösning:}\par
\begin{equation*}
  \begin{gathered}
    F_{\sqrt{X}}(t) = P(\sqrt{X}\leq t) = \begin{cases}0,\;t\leq0\\P(X\leq t^2),\; t>0\end{cases}\\
    = \begin{cases}F_X(t^2),\;t>0\\0,\;t\leq0\end{cases}\Rightarrow f_{\sqrt{X}}(t) = \begin{cases}\dfrac{d}{dt}F_X(t^2),\;t>0\\0,\;t\leq0\end{cases}\\
    = \begin{cases}2tf_X(t^2),\;t>0\\0,\;t\leq0\end{cases} = \begin{cases}2\lambda te^{-\lambda t^2},\;t>0\\0,\;t\leq0\end{cases}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Övning:}\par
\noindent Beskriv fördelningen till $X^2$ och $X^3$ om $X\sim Exp(\lambda)$ eller om $X\sim N(0,1)$
\par\bigskip
\noindent Om $X\sim Exp(\lambda)$ så gäller (betingad):
\begin{equation*}
  \begin{gathered}
    P(X>s+t\;|\;X>s) = P(X>t)\qquad (s,t\leq0)
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent (Minneslöshet kallas detta även)
\par\bigskip
\noindent\textbf{Lösning:}\par
\begin{equation*}
  \begin{gathered}
    P(X>s+t\;|\;X>s) = \dfrac{P(X>s+t\:\&\: X>s)}{P(X>s)}\\
    = \dfrac{P(X>s+t)}{P(X>s)} = \dfrac{1-F_X(s+t)}{1-F_X(s)} = \dfrac{e^{-\lambda(s+t)}}{e^{-\lambda s}} = e^{-\lambda t} = 1-F_X(t) = P(X>t)
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Exempel på minnelösa slumpvariabler}\par
\noindent Om $X = $ tiden det tar innan nästa kund kommer in på ICA så är det minneslöst. Slumpvariabeln är fördelad på samma sätt även om vi hoppar fram i tiden. 
\par\bigskip
\noindent Ett annat exempel kan vara tiden innan det ringer i mobilen, är också minneslöst
\par\bigskip
\noindent Däremot, om $Z = $ tiden jag lever är ej minneslöst, om jag lever till minst 100 år, så är sannolikheten att jag lever till 150år lite annorlunda än sannolikheten att jag lever till 50år. 
\newpage
\begin{theo}
  OOm $P(X>s+t\:|\:X>s) = P(X>t)$ för alla $s\geq0$ och $t\geq0$ så är $X$ exponentialfördelad.
\end{theo}
\par\bigskip
\begin{prf}
  LLåt $\varphi(t) = P(X>t)$\par
  \noindent Då kommer alltså $P(X>s+t\:|\:X>s) = P(X>t)\Lrarr \varphi(s+t) = \varphi(s)\varphi(t)$
  \par\bigskip
  \noindent Vi ska försöka hitta $\varphi$ som löser ovanstående ekvationen. Notera monotont ökande.
  \par\bigskip
  \noindent Sätt $q = \varphi(1)$, vad är då $\varphi(2)?$:
  \begin{equation*}
    \begin{gathered}
      \varphi(2) = \varphi(1+1) = \varphi(1)\varphi(1) = q^2\\
      n\in\Z_+\Rightarrow \varphi(n) = \varphi(\underbrace{1+\cdots+1}_{\text{$n$ st}}) = \varphi(1)^n = q^n
    \end{gathered}
  \end{equation*}\par
  \noindent Vi vet att $\varphi$ tar heltalen som input, men vad är $\varphi(1/n)$?:
  \begin{equation*}
    \begin{gathered}
      \varphi\left(\dfrac{1}{2}\right) = \sqrt{q} = q^{1/2}
    \end{gathered}
  \end{equation*}\par
  \noindent På samma sätt är $\varphi(1)$:
  \begin{equation*}
    \begin{gathered}
      \varphi\underbrace{\left(\dfrac{1}{n}+\cdots+\dfrac{1}{n}\right)}_{\text{$n$ termer}} = \varphi\left(\dfrac{1}{h}\right)^n \Rightarrow \varphi\left(\dfrac{1}{n}\right) = q^{1/n}\\
      \dfrac{m}{n}\in\Q_{>0}\Rightarrow \varphi\left(\dfrac{m}{n}\right) = \varphi\left(\dfrac{1}{n}\right)^m = (q^{1/n})^m = q^{\dfrac{m}{n}}
    \end{gathered}
  \end{equation*}
  \par\bigskip
  \noindent Alltså $P(X>t) = q^t$ om $t\in\Q_{>0}$ så $F_X(t) = 1-q^t$ för $t\in\Q_{>0}$
  \par\bigskip
  \noindent Sätt $q = e^{-\lambda}$ för $\lambda>0\Rightarrow F_X(t) = 1-e^{-\lambda t}$
  \par\bigskip
  \noindent Vi har bara hittat funktionen för rationella tal, vad kan vi göra då?\par
  \noindent Vi vet att fördelningsfunktionen är högerkontinuerlig och monotont ökande, så för $t>0$ $(t\in\R)$, ta rationella tal $q_n>t$ så att $q_n\to t$\par
  \noindent Eftersom fördelningsfunktionen är högerkontinuerlig så måste $F_X(q_n)\to F_X(t)$:
  \begin{equation*}
    \begin{gathered}
      1-e^{-\lambda q_n}\stackrel{n\to\infty}{\to}1-e^{-\lambda t}
    \end{gathered}
  \end{equation*}
  \par\bigskip
  \noindent $F_X(t) = 1-e^{-\lambda t}$ för $t\leq0$, så $X\sim Exp(\lambda)$, men den är noll för alla negativa tal också 
\end{prf}
\newpage
\noindent\textbf{Exempel:}\par
\noindent På ICA kommer det in 20 kunder per timme.\par
\noindent Om vi sätter $X = $ tiden innan nästa kund kommer in, så är $X\sim Exp(\lambda)$, vad är då paramtern $\lambda$?
\par\bigskip
\noindent Väntevärdet kommer vara $E(X) = \dfrac{1}{20} = \dfrac{1}{\lambda}\Lrarr \lambda = 20$\par
\noindent $X$ är alltså exponentialfördelad med $\lambda = 20$
\par\bigskip
\noindent Antalet kunder som dyker upp inom ett visst tidsintervall följer något som kallas för \textit{poisson}-fördelning
\par\bigskip
\noindent Mer nogrannt, säg att vi tal en massa slumpvariabler $X_1,X_2,\cdots$ som är normalfördelade med paramter $\lambda$ och de alla är oberoende\par
\noindent Tar vi en variabel $N_t$ så är antalet kunder som dyker upp i intervallet $[0,t]$\par
\noindent Då kommer $N_t$ följa en poisson-fördelning med parameter $\lambda t$
\par\bigskip
\noindent Vi skriver då $X\sim Po(\lambda t)$. Säger vi att $X\sim Po(\lambda)$ så är:
\begin{equation*}
  \begin{gathered}
    P_X(n) = \dfrac{\lambda^n}{n!}e^{-\lambda}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Om $X\sim Po(\lambda)$, vad är då $E(X)$?
\begin{equation*}
  \begin{gathered}
    E(X) = \sum_{n=0}^{\infty}\dfrac{\lambda^n}{n!}e^{-\lambda} = \sum_{n=1}^{\infty}\dfrac{\lambda^n}{(n-1)!}e^{-\lambda}\\
    = \sum_{n=1}^{\infty}\dfrac{\lambda \lambda^{n-1}}{(n-1)!}e^{-\lambda} = \lambda\underbrace{\sum_{n=0}^{\infty}\dfrac{\lambda^n}{n!}e^{-\lambda}}_{\text{=1}} = \lambda
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Övning:}\par
\noindent Visa att $Var(X) = \lambda$
\par\bigskip
\subsection{Minneslösa diskreta variabler}\hfill\\\par
\noindent Om $P(X>m+n\:|\:X>m) = P(X>n)$ för positiva heltal $m,n$ och samma för $X$\par
\noindent Då kan vi igen sätta $\varphi(n) = P(X>n)$ (beviset blir kortare för vi betraktar enbart heltal)
\par\bigskip
\noindent Då kommer $\varphi(n)$ att vara $\varphi(1)^n$ enligt samma argument som tidigare.\par
\noindent Sätt då $\varphi(1) = q\in(0,1)$, då är $\varphi(n) = q^n$, $P(X>n) = q^n$
\par\bigskip
\noindent Vad är då $P_X(n)$?
\begin{equation*}
  \begin{gathered}
    P(X=1) = 1-P(X>1) = 1-q^1 = 1-q = p\in(0,1)
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Vad är då sannolikheten att $X=2$?
\begin{equation*}
  \begin{gathered}
    P(X=2) = 1-P(X=1)-P(X>2) = \overbrace{1-p}^{\text{$q$}}-q^2 = q(1-q) = pq
  \end{gathered}
\end{equation*}
\newpage
\begin{theo}
  OOm vi vill hitta $P(X=n)$ är det $pq^{n-1}$
\end{theo}
\par\bigskip
\begin{prf}
  SStämmer om $n=1$, vi använder induktion efter det. Om det stämmer $\forall k<n$, vad händer då?
  \begin{equation*}
    \begin{gathered}
      P(X=n) = 1-\overbrace{P(X>n)}^{\text{$q^n$}}-P(X<n)\\
      = 1-q^n-\sum_{k=1}^{n-1}P(X=k) = 1-q^n-\sum_{k=1}^{n-1}pq^{k-1}
      = 1-q^n-p\underbrace{\sum_{k=0}^{n-2}q^k}_{\text{geometrisk summa}}\\
      = 1-q^n-p\dfrac{q^{n-1}-1}{\underbrace{q-1}_{\text{$-p$}}} = 1-q^n-(1-q^{n-1}) = q^{n-1}-q^{n} = q^{n-1}\underbrace{(1-q)}_{\text{$p$}}\\
      = pq^{n-1}
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\noindent Exempel på en sådan är singla slant
\par\bigskip
\begin{theo}[Första gången fördelad]{thm:firsttimevirgin}
  $X$ kallas för \textit{första gången fördelad} ($X\sim \text{ffg}(p)$) om $P(X=n) = p(1-p)^{n-1}$
\end{theo}
\par\bigskip
\noindent\textbf{Exempel:}\par
\noindent Om $X_1,X_2,\cdots\sim Be(p)$ och är oberoende, då är $X = \min\left\{n:X_n=1\right\}$, med andra ord $X(\omega) = \min\left\{n:X_n(\omega) = 1\right\}$ \par
\noindent Då är $X\sim\text{ffg}(p)$, varför det?
\begin{equation*}
  \begin{gathered}
    P(X=n) = P(X_1=0,\cdots,X_{n-1}=0, X_n=1) = \underbrace{P(X_1=0)}_{\text{$1-p$}}\cdots \underbrace{P(X_{n-1}=0)}_{\text{$1-p$}\underbrace{P(X_{n}=1)}_{\text{$p$}}}\\
    = p(1-p)^{n-1}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Övning:}\par
\noindent Visa att $E(X) =\dfrac{1}{p}$ samt att $Var(X) = \dfrac{1-p}{p^2}$
