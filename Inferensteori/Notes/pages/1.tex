\section{Data Analysis (K6)}\par
\noindent Vi kommer undersöka statistisk säkerställd skillnad (Opinion polls example), hypotestestning (räknar sannolikheten att hypotesen är sann).
\par\bigskip
\noindent\textbf{Anmärkning:}\par
\noindent Vanligtvis antar vi att datan är normalfördelad, men inte i alla fall (såsom stickprov av lön)
\par\bigskip
\subsection{Location Measures}\hfill\\\par
\noindent A data set is given by $x_1,\cdots,x_n$
\par\bigskip
\begin{theo}[Sample mean]{thm:smm}
  Sample mean is given by:
  \begin{equation*}
    \begin{gathered}
      \overline{x} = \dfrac{1}{n}\sum_{i=1}^{n}x_i
    \end{gathered}
  \end{equation*}
\end{theo}
\par\bigskip
\begin{theo}[Median]{thm:median}
  The "middle value" of the sorted data. Different from the mean.\par
  \noindent If $n$ is even, the median is defined as the mean of the two middle values
\end{theo}
\par\bigskip
\begin{theo}[Mode]{thm:mode}
  This doesnt work if its continous data but it can be made discrete (such as age/time)\par
  \noindent Mode is the most common data value
\end{theo}
\par\bigskip
\noindent\textbf{Example:}\par
\noindent Let our data points be:
\begin{equation*}
  \begin{gathered}
    32\; 34\; 41\; 44\; 45\; 50\; 50\; 54\; 55\; 57\; 58\; 60\; 63
  \end{gathered}
\end{equation*}\par
\noindent Find mean, median mode:\par
\textit{Mean:} 13 data sets $\Rightarrow n = 13$:
\begin{equation*}
  \begin{gathered}
    \dfrac{1}{13}(32+34+41+44+45+50+50+54+55+57+58+60+63) \approx 49.46
  \end{gathered}
\end{equation*}\par
\textit{Median:} The middle value is 50\par
\textit{Mode:} 50 is the only datavalue appearing more than once. 
\par\bigskip
\noindent\textbf{Anmärkning:}\par
\noindent In this example, the median = mode. This is not always the case!
\par\bigskip
\subsection{Dispersion measures}\hfill\\\par
\noindent Describes the "spread" of the data, such as the variance. We have the following:
\par\bigskip
\begin{theo}[Sample variance]{thm:samplevariance}
  The sample variance is given by:
  \begin{equation*}
    \begin{gathered}
      s^2 = \dfrac{1}{n-1}\sum_{i=1}^{n}(x_i-\overline{x})^2
    \end{gathered}
  \end{equation*}
\end{theo}
\newpage
\begin{theo}[Sample standard variance]{thm:samplestdvar}
  Is given by:
  \begin{equation*}
    \begin{gathered}
      s = \sqrt{\dfrac{1}{n-1}\sum_{i=1}^{n}(x_i-\overline{x})^2}
    \end{gathered}
  \end{equation*}
  \par\bigskip
  \noindent Also called \textit{sample standard deviation}
\end{theo}
\par\bigskip
\begin{theo}[Range]{thm:range}
  Variationsbredden. The differnece between the largest and the smallest values of the data 
\end{theo}
\par\bigskip
\begin{theo}[Inter quartile range]{thm:quartilrange}
  Kvartilavståndet is the difference between the upper and lower quartiles.\par
  \noindent If we have an odd amount of data it is including the median!
\end{theo}
\par\bigskip
\begin{theo}[Mid-range]{thm:midrange}
  The mean between the biggest and smallest value in the sample
\end{theo}
\par\bigskip
\begin{theo}[Lower/Upper quartile]{thm:quartile}
  The \textit{lower quartile} is the median of the lower half of the data material including the median if $n$ is odd
  \par\bigskip
  \noindent The \textit{upper quartile} is the median of the upper half of the data material including the median if $n$ is odd
\end{theo}
\par\bigskip
\noindent\textbf{Example:}\par
\begin{equation*}
  \begin{gathered}
    0\;0\;1\;1\;2\;2
  \end{gathered}
\end{equation*}\par
\noindent Here, the mean is given by $\dfrac{(1+1+2+2)}{6} = 1$.\par
\noindent Therefore, the sample variance is given by $\dfrac{4}{5}$ and the sample standard deviation $\sqrt{\dfrac{4}{5}}$\par
\noindent We can find the inter quartile range by looking at the half, like this:
\begin{equation*}
  \begin{gathered}
    [0\;\underbrace{0}_{\text{$\Delta$}}\;1]\;[1\;\underbrace{2}_{\text{$\Delta$}}\;2]
  \end{gathered}
\end{equation*}\par
\noindent Therefore, the inter quartile range here is $2-0=2$
\par\bigskip
\subsection{Graphical illustration}\hfill\\\par
\noindent\textbf{Stem av leafplots:}
\begin{verbatim}
u = c(32,34,...)
stem(u)
\end{verbatim}
\par\bigskip
\noindent\textbf{Boxplots:}\par
\noindent Uses quartiles, max min, and median. Useful if you want a quick look at the dispersion of data.
\par\bigskip
\noindent\textbf{Bar chart:}\par
\noindent Good for illustrating the frequency of each data point, but for large data points the data is hard to read
\par\bigskip
\noindent\textbf{Histogram:}\par
\noindent Attemps to fix the readability issues with the bar chart and is easier to compare with probability density functions.\par
\noindent Easier to manipulate data for readability (use bigger/smaller intervals) (one can ask what the optimal width for a histogram would be)\par
\noindent Very often you can ask if the data follow a normal distribution, which can be hard by just looking at the histogram (because the width varies) 
\par\bigskip
\noindent\textbf{Thoughts:}\par
\noindent Dynamically widths on histograms, the more sparse data the greater the width and the more dense, the less the width 
\par\bigskip
\noindent\textbf{QQ-plot}:\par
\noindent Is the data normally distributed? You order your data and construct a table with your data and compare it with if it was normally distributed:
\begin{equation*}
  \begin{gathered}
    \Phi(z) = \dfrac{i-0.5}{n}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent If data was perfectly normal on both axis, $x_i$ would be a linear funcfion of $z$, ie. normally distributed $N(0,1)$\par
\noindent We plot $z$ on the $x$-axis and $x_i$ on the $y$-axis
\par\bigskip
\noindent The name comes from quantile-quantile-plot (QQ-plot). It is a graphical way of comparing two probability distributions (sannolikhetsfördelning) 
\subsection{Data materials in several dimensions}\hfill\\\par
\noindent We can calculate correlation through sample covariance:
\par\bigskip
\begin{theo}[Sample covariance]{thm:samplecovar}
  \begin{equation*}
    \begin{gathered}
      c_{xy} = \dfrac{1}{n-1}\sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y})
    \end{gathered}
  \end{equation*}
\end{theo}\par
\noindent\textbf{Anmärkning:}\par
\noindent Not scale invariant (if you measure $x$ in meters and go to cm then it is not the same). Therefore we need to norm it with something, which is where the correlation comes in:
\par\bigskip
\begin{theo}[Sample correlation coefficient]{thm:scc}

  \begin{equation*}
    \begin{gathered}
      r_{xy} = \dfrac{c_{xy}}{s_xs_y}
    \end{gathered}
  \end{equation*}\par
  \noindent Where $s_x$ and $s_y$ are the sample standard deviations for $x$ and $y$
\end{theo}
\par\bigskip
\begin{theo}[Sample correlation satisfies]{thm:scs}
  The sample correlation coefficient satisfies
  \begin{equation*}
    \begin{gathered}
      -1\leq r_{xy}\leq 1
    \end{gathered}
  \end{equation*}\par
  \noindent If it is 1, then there is a strong positive correlation (the linear regression has a line with positive derivative), similarly for negative.\par
  \noindent When it is 0 there is no \textit{linear} relation. There might be other, for example quadratic relation.
\end{theo}
\newpage
\begin{prf}[Sample correlation satisfaction]{prf:sccas}
  \begin{equation*}
    \begin{gathered}
      0\leq \dfrac{1}{n-1}\sum_{i=1}^{n}\left(\dfrac{x_i-\overline{x}}{s_x}-\dfrac{y_i-\overline{y}}{s_y}\right)^2\\
      = \dfrac{1}{s_x^2}\underbrace{\dfrac{1}{n-1}\sum_{i}\left(x_i-\overline{x}\right)^2}_{\text{$s_x^2$}}+\dfrac{1}{s_y^2}\underbrace{\dfrac{1}{n-1}\sum_{i}(y_i-\overline{y})^2}_{\text{$s_y^2$}}-2\dfrac{1}{s_xs_y}\underbrace{\dfrac{1}{n-1}\sum_{i}(x_i-\overline{x})(y_i)(\overline{y})}_{\text{$c_{xy}$}}\\
      = 2-2r_{xy}\Rightarrow r_{xy}\leq1\\
      0\leq\dfrac{1}{n-1}\sum_{i=1}^{n}\left(\dfrac{x_i-\overline{x}}{s_x}+\dfrac{y_i-\overline{y}}{s_y}\right)^2 = 2+2r_{xy}\\
      \Rightarrow -1\leq r_{xy}
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\begin{prf}[Sample correlation satsifaction]{prf:sccas2}
  There is yet another proof that may be more intuitive for the boundedness of the sample correlation coefficient satisfaction that goes as follows:
  \par\bigskip
  \noindent Recall that the covariance is an inner-product, and that the variance $Var(x) = Cov(x,x)$
  \par\bigskip
  \noindent The "worst" that can happen is that $x,y$ are independent, whereby the covariance $Cov(x,y) = E(XY)-E(X)E(Y) = 0$ since $E(XY) = E(X)E(Y)$
  \par\bigskip
  \noindent This gives the following:
  \begin{equation*}
    \begin{gathered}
      \left|\dfrac{c_{x,y}}{s_xs_y}\right| = \left|\dfrac{Cov(x,y)}{\sqrt{Var(x)}\sqrt{Var(y)}}\right|\geq0
    \end{gathered}
  \end{equation*}
  \par\bigskip
  \noindent Next, we will use the fact that the covariance is an inner product, and therefore the Cauchy-Schwartz inequality holds\par
  \noindent Notice that $Cov(x,x) = Var(x)$
  \begin{equation*}
    \begin{gathered}
      \left|Cov(x,y)\right|^2\leq\left|Var(x)Var(y)\right|\Lrarr \left|Cov(x,y)\right|\leq \left|\sqrt{Var(x)}\sqrt{Var(y)}\right|\\
      = \left|\dfrac{Cov(x,y)}{\sqrt{Var(x)}\sqrt{Var(y)}}\right|\leq 1
    \end{gathered}
  \end{equation*}
  \par\bigskip
  \noindent From the properties of absolute values, we therefore get $-1\leq r_{x,y}\leq 1$ 
\end{prf}
