\section{Laws \& Distribution functions}
\par\bigskip
\begin{defo}[Law]{}
  Let $X$ be a random variable on $(\Omega, \mathcal{F},\P)$. A \textit{law} $\mathcal{L}_X(A)$ captures probability of $X$ in $A$, where $A\in\mathcal{B}\left(\R\right)$
  \begin{equation*}
    \begin{gathered}
    \mathcal{L}_X(A) = \P(X^{-1}(A)) = \P(\left\{\omega\in\Omega:X(\omega)\in A\right\})
    \end{gathered}
  \end{equation*}
\end{defo}
\par\bigskip
\noindent\textbf{Remark:}\par
\noindent This is the pull-back measure on $\R$. It is uniquely characterized by something known as the cummulative distribution function
\begin{equation*}
  \begin{gathered}
    F_X(t) = \P(\left\{X\leq t\right\}) = \mathcal{L}_X((-\infty,t])
  \end{gathered}
\end{equation*}
\par\bigskip
\subsection{Properties of distribution functions}\hfill\\
\begin{itemize}
  \item Non-decreasing, i.e $F_X(t)\geq F_X(s)$ if $t\geq s$
  \item $\lim_{t\to-\infty}F_X(t) = 0$ $\lim_{t\to\infty}F_X(t) = 1$
  \item Right continuous, i.e $\lim_{t\searrow a}F_X(t) = F_X(a)$
\end{itemize}\par
\noindent Conversally, any $F$ satisfying the above gives rise to a probability measure $\mathcal{L}$ such that $\mathcal{L((\infty,t])}$ is just the value at $t$
\par\bigskip
\section{Independence}\par
\noindent In this little mini-chapter, we fix a probability space $(\Omega, \mathcal{F}, \P)$
\par\bigskip
\begin{defo}[Independence]{}
  Let $E_1,\cdots,\in\mathcal{F}$ be events (finitely many or countably infinite).\par
  \noindent We say these are \textit{independent} if for any combination of the following:
  \begin{equation*}
    \begin{gathered}
      \P(E_{i_1}\cap\cdots\cap E_{i_k}) = \prod_{j=1}^{k}\P(E_{e_j})\quad\forall i_1<\cdots<i_k\wedge k
    \end{gathered}
  \end{equation*}
\end{defo}
\par\bigskip
\noindent\textbf{Example:}\par
\noindent Consider throw of die as before. Let $E_1 = $ "number $\leq 2$" $=\left\{1,2\right\}$.\par
\noindent Here $\P(E_1) = \dfrac{2}{6} = \dfrac{1}{3}$. Let $E_2$ be "number we roll is even" $ = \left\{2,4,6\right\}$\par
\noindent $\P(E_2) = \dfrac{3}{6} = \dfrac{1}{2}$. Then $\P(E_1\cap E_2) = \P(\left\{2\right\}) = \dfrac{1}{6} = \dfrac{1}{2}\cdot\dfrac{1}{3} = \P(E_1)\P(E_2)\Rightarrow$ independence.
\par\bigskip
\noindent\textbf{Example:}\par
\noindent Let $E_3 = \text{"number "}\leq3 =\left\{1,2,3\right\}$, $\P(E_3) = \dfrac{1}{2}$.\par
\noindent Then $\P(E_2\cap E_3) = \P(\left\{2\right\}) = \dfrac{1}{6} \neq \dfrac{1}{2}\cdot\dfrac{1}{2}= \P(E_2)\P(E_3)\Rightarrow E_2\wedge E_3$ are independent.
\par\bigskip
\begin{defo}[Independent random variable]{}
  Let $X_1,\cdots$ be finite (or countably infinite) random variables. We say that these are \textit{independent} if all events that can happen with these random variables are independent, i.e for any choice $i_1<\cdots<i_k$ and Borel sets $A_1,\cdots, A_k\in\mathcal{B}\left(\R\right)$
  \begin{equation*}
    \begin{gathered}
      \P(\left\{X_{i_1}\in A_1\right\}\cap \left\{X_{i_2}\in A_2\right\}\cap\cdots\cdot\cap\left\{X_{i_k}\in A_k\right\}) = \prod_{j=1}^{k}\P(\left\{X_{ij}\in A_j\right\})
    \end{gathered}
  \end{equation*}
\end{defo}\par
\noindent A little remark may be that it looks kinda like a $\pi$-system. Can we define this using $\sigma$-algebras?
\par\bigskip
\begin{defo}[]{}
  The sub $\sigma$-algebras $\mathcal{G}_1,\mathcal{G}_2,\cdots$ (finitely many or countably infinite) are said to be independent if
  \begin{equation*}
    \begin{gathered}
      \P(G_{i_1}\cap\cdots\cap G_{i_k}) = \prod_{j=1}^{k}\P(G_{i_j})\quad\forall G_{i_j}\in\mathcal{G}_{i_j}
    \end{gathered}
  \end{equation*}
\end{defo}
\par\bigskip
\noindent\textbf{Remark:}\par
\noindent Note that independence of events and random vairables are special cases\par
\begin{itemize}
\item Let $E_1,\cdots$ be events. We let $\mathcal{G}$ be the trivial ones generated by an event, i.e $\mathcal{G}_{i_j} = \left\{\varnothing, \Omega, E_i,E_i^c\right\}$
\item Let $X_1,\cdots$ be random variables and let $\mathcal{G}_i = \sigma(X_i)$
\end{itemize}
\par\bigskip
\begin{lem}[]{}
  Let $\mathcal{G},\mathcal{H}$ be the sub $\sigma$-algebras and $\mathcal{I}, \mathcal{J}$ be 2 $\pi$-systems (i.e invariant under intersections) such that $\sigma(\mathcal{I}) = \mathcal{G}$ and $\sigma(\mathcal{J}) = \mathcal{H}$
  \par\bigskip
  \noindent Thus, $\mathcal{G}, \mathcal{H}$ are independent $\Lrarr$ $\P(I\cap J) = \P(I)\P(J)\quad\forall I\in\mathcal{I}, J\in\mathcal{J}$
\end{lem}
\par\bigskip
\noindent\textbf{Remark:}\par
\begin{itemize}
\item$\left\{E_i\right\}$ are $\pi$-systems
\item Events on the form $\left\{X_i\leq t\right\}$ are $\pi$-systems from $\sigma(X_i)$
\end{itemize}\par
\noindent To verify these claims, it suffices to check 
\begin{equation*}
  \begin{gathered}
    \P(X_{i_1}\leq x_1\quad\&\quad X_{i_2}\leq x_2\cdots) = \prod_{j=1}^{k}\P(X_{i_j}\leq x_j)
  \end{gathered}
\end{equation*}
\par\bigskip
\begin{theo}[Second Borell-Cantelli Theorem]{}
  Assume $E_1,\cdots$ are independent events and $\sum_{j=1}^{\infty}\P(E_j) =\infty$. Then
  \begin{equation*}
    \begin{gathered}
      \P(\lim_{n\to\infty}\sup E_n) = \P(E_n\quad\text{ happens $\infty$ often}) = 1
    \end{gathered}
  \end{equation*}
\end{theo}
\newpage
\begin{prf}[]{}
  Recall, $\lim_{n\to\infty}E_n = \bigcap_{n\in\N}\bigcup_{m\geq n} E_m$. Our strategy will be to prove that the complement is 0 (this is a good strategy whenever we want to prove that the probability of something is 1), since we can find the upper bound to be infinitely small.
  \par\bigskip
  \noindent By de-Morgan, complement is given by
  \begin{equation*}
    \begin{gathered}
      \bigcup_{n\in\N}\bigcap_{m\geq n} E_m^c
    \end{gathered}
  \end{equation*}\par
  \noindent Now:
  \begin{equation*}
    \begin{gathered}
      \P(\bigcap_{m\geq n}^{M}E_m^c)\geq\P(\bigcap_{n\leq m\geq M}E_m^c) = \prod_{m=n}^{M}\P(E_m^c)\\
      = \prod_{m=n}^{M}1-\P(E_m)
    \end{gathered}
  \end{equation*}\par
  \noindent (Complements preserves independence). We approximate using exponents since 
  \begin{equation*}
    \begin{gathered}
      1-x\leq e^{-x}\Lrarr \ln{\left(t\right)}<t-1\leq \prod_{m=n}^{M}e^{-\P(E_m)} = \text{exp}\left\{-\sum_{m=n}^{M}\P(E_m)\right\}
    \end{gathered}
  \end{equation*}\par
  \noindent Since $\P(E_m)\to\infty$ as $M\geq n\to0$
  \par\bigskip
  \noindent Taking $M\gg n$, we get
  \begin{equation*}
    \begin{gathered}
      \P(\bigcap_{m>n}E_m^c)<\varepsilon\quad\forall \varepsilon>0\Rightarrow \P(\bigcap_{m>n}E_m^c) =0\\
      \Rightarrow \begin{rcases}\underbrace{\bigcup_{n\in\N}\underbrace{\bigcap_{m\geq n}E_m^c}_{\P=0}}_{\P=0}\end{rcases}\text{ countable unions of something with $\P=0$ yields 0}\\
      \Rightarrow \text{Complement is 1}
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\noindent As long as the sum tends to 0 such that the sum diverges, it will happen infinitely often.
\par\bigskip
\noindent\textbf{Example:}\par
\noindent Let $E_k = $ "draw card 1 on $k$th draw". Assuming independence, we have
\begin{equation*}
  \begin{gathered}
    \P(E_k) = \dfrac{1}{k} \quad\wedge\quad \sum_{k=1}^{\infty}\P(E_k) = \sum_{k=1}^{\infty}\dfrac{1}{k}
  \end{gathered}
\end{equation*}\par
\noindent By the second Borell-Cantelli theorem, $\P(\text{"draw 1 $\infty$ often"}) = 1$
\par\bigskip
\noindent\textbf{Remark:}\par
\noindent For $E_1,\cdots$ independent events, the Borell-Cantelli theoerems give the following dichotomy:
\begin{equation*}
  \begin{gathered}
    \P(\lim_{n\to\infty}\sup E_n) = \P(E_n\text{ happens $\infty$ often}) = \begin{cases}1\quad\text{if } \sum_i \P(E_i) = \infty\\0\quad\text{if}\sum_i\P(E_i)<\infty\end{cases}
  \end{gathered}
\end{equation*}\par
\noindent This is a special case of Kolmogorovs 0-1 Law:
\par\bigskip
\begin{defo}[Tail $\sigma$-algebra]{}
  Let $X_1,\cdots$ be a sequence of random variables. Set $\mathcal{T}_n = \sigma(X_{n+1},X_{n+2},\cdots)$ and $\mathcal{T} = \bigcap_{n\in\N}\mathcal{T}_n$\par
  \noindent What remains is knowledge at $\infty$. We say $\mathcal{T}$ is a tail $\sigma$-\textit{algebra}
\end{defo}
\par\bigskip
\noindent\textbf{Example:}\par
\noindent Some typical events $E\in\mathcal{T}$ are $\left\{\lim_{n}X_n\text{ exists}\right\}$ or something like $\left\{\sum X_n\text{ converges}\right\}$.\par
\noindent The point here is that all information is talking about what happens at $\infty$
\par\bigskip
\begin{theo}[Kolmogorovs 0-1 Law]{}
  Let $X_1,\cdots$ be independent random variables. Then $\forall T\in\mathcal{T}$ (tail $\sigma$-algebras generated by $X_i$), we have either $\P(T) = 0$ or $\P(T) = 1$ almost surely.
\end{theo}
\par\bigskip
\begin{prf}[(Sketch) Kolmogorovs 0-1 Law]{}
  \begin{enumerate}[leftmargin=*]
    \item Define $\mathcal{X}_n = \sigma(X_1,\cdots, X_n)$. From this, we can say that the $\mathcal{X}_n$ and $\mathcal{T}_n$ are indeoendent (one stops at $n$, the other one continues at $n+1$) $\forall n\in\N$
    \item $\mathcal{T}\subseteq\mathcal{T_n}\quad\forall n$, so $\mathcal{T}$ is independent of $\mathcal{X}_n\quad\forall n$ 
    \item $\mathcal{X}_\infty = \sigma(X_1,\cdots)$ and $\mathcal{T}$ must be independent since $\mathcal{T}$ is independent for all $\mathcal{X}_n$ and $\bigcup \mathcal{X}_n$ are a $\pi$-system (which generates $\mathcal{X}_\infty$)
    \item $\mathcal{T}\subseteq\mathcal{X}_\infty$ (knowledge in tail is contained in $\infty$ for $\mathcal{X}$)
  \end{enumerate}
  \par\bigskip
  \noindent So for any event $F\in\mathcal{T}$ we know
  \begin{equation*}
    \begin{gathered}
      \P(F\cap F) = \P(F) = \P(F)\P(F)\\
    \end{gathered}
  \end{equation*}\par
  \noindent$\Rightarrow\P(F)$ must solve $x = x^2$, i.e $x\in \left\{0,1\right\}$
\end{prf}
\par\bigskip
\noindent\textbf{Corollary:}\par
\noindent Let $\xi$ be a $\mathcal{T}$ measurable random variable (random thing that only depends on tail, eg. $x=\begin{cases}1\quad\text{if $\lim_{n\to\infty}$ exists}\\0\quad\text{else}\end{cases}$), then $\exists C\in[-\infty,\infty]$ such that $\P(\xi=C) = 1$, i.e $\xi$ is almost surely constant.
