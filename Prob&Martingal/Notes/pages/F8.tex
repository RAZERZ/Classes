\section{Product Measures \& Product Spaces}
Given 2 measure spaces $(S_1,\Sigma_1,\mu_1)$ and $(S_2,\Sigma_2,\mu_2)$, we want to build a "canonical" measure on $S = S_1\times S_2$\par
\noindent First we build the product $\Sigma$-algebra.
\par\bigskip
\subsection{Product $\Sigma$-algebra}\hfill\\
\noindent The notation we shall use is $\Sigma = \Sigma_1\times\Sigma_2 = \sigma\left(\bigcup_{A\in\Sigma_1}\times S_2\cup\bigcup_{B\in S_2}S_1\times B\right)$\par
\noindent\textbf{Remark:}\par
\noindent It is generated by a $\pi$-system on the form $\left\{A_1\times A_2: A_1\in \Sigma_1, A_2\in \Sigma_2\right\}$
\par\bigskip
\noindent If $f$ is a bounded measurable function on $(S,\Sigma)$, then we have the projection
\begin{equation*}
  \begin{gathered}
    S_1\to\R\qquad s_1\mapsto f(s_1,s_2)\quad\text{fix } s_2\\
    S_2\to\R\qquad s_2\mapsto f(s_1,s_2)\quad\text{fix } s_1\\
    \text{Where $f$ is measurable with respect to $\Sigma_1$ and $\Sigma_2$ resp.}
  \end{gathered}
\end{equation*}
\par\bigskip
\begin{prf}[]{}
  It holds for indicator functions on the form $I_{A_1\times A_2}(s_1,s_2) =\begin{cases}1\quad\text{if } (s_1,s_2)\in A_1\times A_2\\0\quad\text{else}\end{cases}$
  \par\bigskip
  \noindent Then we can extend this
\end{prf}
\par\bigskip
\subsection{Product Measures}\hfill\\
\noindent The goal is to define a measure that works with projections.\par
\noindent Assume we are given 2 measures $\mu_1,\mu_2$ on $(S_1,\Sigma_2)$ and $(S_2,\Sigma_2)$
\begin{equation*}
  \begin{gathered}
    \mathcal{I}_1^f(s_2) = \int_{S_2}f(s_1,s_2)d\mu_2\qquad\mathcal{I}_2^f(s_2) = \int_{S_1}f(s_1,s_2)d\mu_1
  \end{gathered}
\end{equation*}
\par\bigskip
\begin{lem}[]{}
  If $f$ was bounded and measurable, then $\mathcal{I}_1,\mathcal{I}_2$ are bounded and measurable
\end{lem}
\par\bigskip
\begin{prf}[]{}
  We use indicators!
  \begin{equation*}
    \begin{gathered}
      f = I_{A_1\times A_2},\quad \mathcal{I}_1^f(s_1) = \int_{S_2}I_{A_1\times A_2}(s_1,s_2)d\mu_2 = \int_{S_2}I_{A_1}(s_1)I_{A_2}(s_2)d\mu_2\\
      = I_{A_1}(s_1)\int_{S_2}I_{A_2}(s_2)d\mu_2 = I_{A_1}\int_{A_2}d\mu_2 = I_{A_1}(s_1)\mu_2(A_2)
    \end{gathered}
  \end{equation*}\par
  \noindent Which is bounded and measurable. Similarly proceed for $\mathcal{I}_2^f$ and for an arbitrary $f$
\end{prf}
\par\bigskip
\noindent Now for $F\in\Sigma$ and $f = I_{F}(s_1,s_2)$. Define measure of $f$  by
\begin{equation*}
  \begin{gathered}
    \mu(F) = \int_{S_1}\mathcal{I}_1^fd\mu_1 = \int_{S_1}\int_{S_2}f(s_1,s_2)d\mu_2d\mu_1 = \int_{S_2}\mathcal{I}_2^f(s_2)d\mu_2 = \int_{S_2}\int_{S_1}f(s_1,s_2)d\mu_1d\mu_2
  \end{gathered}
\end{equation*}
\par\bigskip
\begin{theo}[Fubinis]{}
  The measure of $f$ for $F\in\Sigma$ and $f = I_{F}(s_1,s_2)$ is well defined and you may indeed swap orders of integrals. In fact, what we end up with is:
  \begin{equation*}
    \begin{gathered}
      \int_{S_1}\int_{S_2}fd\mu_2d\mu_1 = \int_{S_2}\int_{S_1}fd\mu_1d\mu_2 =\int_S fd\mu
    \end{gathered}
  \end{equation*}\par
  \noindent For all non-negative (MCT) integrable (DCT) $f$
\end{theo}
\par\bigskip
\begin{prf}[Fubinis Theorem]{}
  We consider $f = I_{A_1\times A_2}$ and 
  \begin{equation*}
    \begin{gathered}
      \int_{S_1}\int_{S_2}I_{A_1\times A_2}(s_1,s_2)d\mu_2d\mu_1 = \int_{S_1}\int_{S_2}I_{A_1}(s_1)I_{A_2}(s_2)d\mu_2d\mu_1 = \int_{S_1}I_{A_1}(s_1)d\mu_1\int_{S_2}I_{A_2}(s_2)d\mu_2
    \end{gathered}
  \end{equation*}\par
  \noindent By symmetry of multiplication:
  \begin{equation*}
    \begin{gathered}
      =\underbrace{\int_{S_2}I_{A_2}(s_2)d\mu_2}_{\mu_2(A_2)}\underbrace{\int_{S_1}I_{A_1}(s_1)d\mu_1}_{\mu_1(A_1)} = \int_{S_2}\int_{S_1}fd\mu_1d\mu_2
    \end{gathered}
  \end{equation*}\par
  \noindent For general (non-indicators), we approximate by step functions
\end{prf}
\par\bigskip
\noindent Generally, $\mu(A_1\times A_2) = \mu_1(A_1)\mu_2(A_2)$. In fact, $\mu$ is uniquely defined by this relationship since 
\begin{equation*}
  \begin{gathered}
    \left\{A_1\times A_2:A_1\in\Sigma_1,A_2\in\Sigma_2\right\}
  \end{gathered}
\end{equation*}is defined by a $\pi$-system.\par
\noindent This construction defined $\mu = \mu_1\times\mu_2$
\par\bigskip
\noindent\textbf{Remark:}\par
\noindent Fubini extends to $\sigma$-finite measures, but does not necessarily hold for non-$\sigma$-finite measures.
\par\bigskip
\noindent\textbf{Example:}\par
\noindent $(s_1,\mu_1) = [0,1]$, and $\mu_1 = $ Lebesgue on $[0,1]$ (this is $\sigma$-finite)\par
\noindent $(s_2,\mu_2) = [0,1]$ and $\mu_2 =$ counting measure (this is not $\sigma$-finite)\par
\noindent Lets check if $\mu = \mu_1\times\mu_2$ will still hold with Fubini:\par
\noindent Let $f(s_1,s_2) = \begin{cases}1\quad\text{if } s_1 = s_2\\0\quad\text{else}\end{cases}$, but:
\begin{equation*}
  \begin{gathered}
  \int_{S_1}\int_{S_2}fd\mu_2d\mu_1 =\int_{S_1}1d\mu_1 = 1\quad\text{ since counting measure on $\left\{1\right\}$ is 1}\\
  \int_{S_2}\int_{S_1}fd\mu_1d\mu_2 = \int_{S_2}0d\mu_2 = 0
  \end{gathered}
\end{equation*}\par
\noindent Since $0\neq1$ Fubinis theorem does \textit{not} hold.
\par\bigskip
\noindent\textbf{Remark:}\par
\noindent We can iterate construction to define product measures on the form $\mu = \mu_1\times\cdots\times\mu_n$, but in fact, construction holds for countable products
\begin{equation*}
  \begin{gathered}
    \mu(A_1\times\cdots\times A_k\times S_{k+1}\times\cdots) = \mu(A_1)\mu(A_2)\cdots\mu(S_{k+1})
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Example:}\par
\noindent The $d$-dimensional Lebesgue measure $\mathcal{L}^d$ can be defined by $\underbrace{\mathcal{L}^1\times\cdots\times\mathcal{L}^1}_{\text{$d$-times}}$
\par\bigskip
\subsection{Application}\hfill\\
\noindent We can construct a formula for expectation of $X$. Suppose we have $X$ be a non-negative random variable on $(\Omega,\mathcal{F},\P)$, then
\begin{equation*}
  \begin{gathered}
    \iint_\Omega \underbrace{I(X\geq x)d\P}_{\text{expectation of indicator}}dx = \int_{0}^{\infty}\P(X\geq x)dx
  \end{gathered}
\end{equation*}\par
\noindent By Fubinis theorem:
\begin{equation*}
  \begin{gathered}
    \int_\Omega\underbrace{\int_{0}^{\infty}I(X\geq x)dxd\P}_{=X(\omega)} = \int_\Omega X(\omega)d\P = \E(X)
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent We shall consider the special case of product probability measure on $\R\times\R$ with densities $f_X,f_Y$ (componentwise) and $f_{X,Y}$ is the joint density.
\begin{equation*}
  \begin{gathered}
    \P((X,Y)\in A) = \iint_Af_{X,Y}(x,y)dxdy\quad A\in\mathcal{B}(\R^2)
  \end{gathered}
\end{equation*}\par
\noindent We define conditional density through:
\begin{equation*}
  \begin{gathered}
    f_{X\mid Y}(x\mid y) = \begin{cases}\dfrac{f_{X,Y}(x,y)}{f_Y(y)}\quad\text{ if } f_Y(y)\neq0\\0\quad\text{ else}\end{cases}
  \end{gathered}
\end{equation*}\par
\noindent Here $f_Y(y) =\int_\R f_{X,Y}(x,y)dx$
\par\bigskip
\noindent For fixed $y$,
\begin{equation*}
  \begin{gathered}
    \int_\R f_{X\mid Y}(x\mid y) = \int_\R f_{X,y}(x,y)/f_Y(y)dx = \dfrac{1}{f_Y(y)}\int_\R f_{X,Y}(x,y)dx = \dfrac{f_Y(y)}{f_Y(y)} = 1
  \end{gathered}
\end{equation*}\par

\noindent when $f_Y = 0$ its 0.\par
\noindent So $f_{X\mid Y(x\mid y)}$ is a denisty (gives rise to a probability measure). A good guess of conditional expectation given some point $Y$:
\begin{equation*}
  \begin{gathered}
    g(y) = \int_R xf_{X\mid Y}(x\mid y) dx = \text{ conditional expectation of $x$ w.r.t some $y$}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent We now want to show that $g(y)$ satisfies conditions of conditional expectation. By inspection, $g$ is $\sigma(Y)$ integrable. We need to show 3), so let $A\in \Sigma_2$, want to show
\begin{equation*}
  \begin{gathered}
    \int_{\left\{Y\in A\right\}}Xd\P = \int_{\left\{Y\in A\right\}}g(Y)d\P
  \end{gathered}
\end{equation*}\par
\noindent LHS:
\begin{equation*}
  \begin{gathered}
    =\int_\Omega I_{Y\in A}Xd\P
  \end{gathered}
\end{equation*}\par
\noindent We use the joint denisty to express as integral over $\R\times \R$:
\begin{equation*}
  \begin{gathered}
    \iint_{R\times R}I_{Y\in A}xf_{X,Y}(x,y)dxdy
  \end{gathered}
\end{equation*}\par
\noindent By Fubinis theorem:
\begin{equation*}
  \begin{gathered}
    =\int_\R x\int_\R I_{Y\in A} f_{X,Y}(x,y)dydx
  \end{gathered}
\end{equation*}\par
\noindent For the RHS, get rid of interval:
\begin{equation*}
  \begin{gathered}
    \int_{\Omega}I_{Y\in A}g(Y)d\P
  \end{gathered}
\end{equation*}\par
\noindent Use the density:
\begin{equation*}
  \begin{gathered}
    \iint_{\R\times \R} I_{Y\in A}g(y)f_{X,Y}(x,y)dxdy
  \end{gathered}
\end{equation*}\par
\noindent By Fubinis theorem:
\begin{equation*}
  \begin{gathered}
    \int_\R\int_A g(y)f_{X,Y}(x,y)dydx
  \end{gathered}
\end{equation*}\par
\noindent By definition of density $f_{X,Y}(x,y)=f_{X\mid Y}(x\mid y)f_Y(y)$ for a set of full measure (this case Lebesgue).\par
\noindent LHS becomes:
\begin{equation*}
  \begin{gathered}
    \int_\R x\int_A f_{X\mid Y}(x\mid y)f_Y(y)dydx
  \end{gathered}
\end{equation*}\par
\noindent By Fubinis theorem:
\begin{equation*}
  \begin{gathered}
    \int_A f_Y(y)\underbrace{\int_\R xf_{X\mid Y}(x\mid y)dx}_{=g(y)}dy = \int_A f_Y(y)g(y)dy = \int_A\left(\int_\R f_{X,Y}(x,y)dx\right)g(y)dy
  \end{gathered}
\end{equation*}\par
\noindent By using Fubinis theorem on the RHS, we finally get RHS = LHS and since $A$ was arbitrarily chosen we have checked 3).
\par\bigskip
\noindent\textbf{Example:}\par
\noindent Consider random variables $X,Y\sim U\left([0,1]\times[0,1]\right)$ only on the lower triangle $f_{X,Y}(x,y) = 2I_{\left\{x\geq y\right\}}$. We get
\begin{equation*}
  \begin{gathered}
    f_Y(y) = \int_{0}^{1}2I_{\left\{x\geq y\right\}}dx = \int_{y}^{1}2dx = 2-2y\\
    f_{X\mid Y}(x\mid y) = \dfrac{2I_{\left\{x\geq y\right\}}}{2-2y} = \dfrac{I_{\left\{x\geq y\right\}}}{1-y}\qquad g(y) = \int_{0}^{1}x\dfrac{I_{\left\{x\geq y\right\}}}{1-y}\\
    = \dfrac{1}{1-y}\int_{y}^{1}xdx = \dfrac{1}{2}\dfrac{(1-y^2)}{1-y} = \dfrac{1+y}{2} = \E(X\mid Y=y)
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Example:}\par
\noindent Let $X_1,\cdots,X_n$ be independent identically distributed random variables and let $S_n = \sum_{i=1}^{n}X_i$. What is $\E(X_1\mid S_n)$? Well let $A\in \sigma(S_n)$, we must have 
\begin{equation*}
  \begin{gathered}
    \int_A \E(X_1\mid S_n) + \E(X_2\mid S_n)+\cdots+\E(X_n\mid S_n) d\P\\
    = \int_A X_1+\cdots+X_n d\P = \int_A S_nd\P
  \end{gathered}
\end{equation*}\par
\noindent So 
\begin{equation*}
  \begin{gathered}
    n\int_A \E(X_1\mid S_n)d\P = \int_A S_nd\P\quad\forall A
  \end{gathered}
\end{equation*}\par
\noindent In particular, we can take our $\Omega$
\begin{equation*}
  \begin{gathered}
    \Rightarrow \E(X_1\mid S_n) = \dfrac{S_n}{n}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Is it possible to derive conditional probablity in terms of conditional epectation?
\begin{equation*}
  \begin{gathered}
    \P(A\mid\mathcal{G}) = \E(I_A\mid\mathcal{G})
  \end{gathered}
\end{equation*}\par
\noindent Where $A\in\Sigma$. Here $\P(A\mid\mathcal{G})$ is a random variable. This is unique since the expectation is unique (up to a null set) and satisfies
\begin{equation*}
  \begin{gathered}
    \P\left(\bigcup_{i=1}^{\infty}A_i\mid\mathcal{G}\right) = \sum_{i=1}^{\infty}\P(A_i\mid\mathcal{G})
  \end{gathered}
\end{equation*}\par
\noindent If $A_i\neq A_j$ for $i\neq j$
\par\bigskip
\noindent If $\mathcal{G} = \sigma(B)$ where $B\in\Sigma$ ($\sigma$-algebra condition on some event), then $\P(A\mid\mathcal{G})$ is a random variable and $\mathcal{G} = \left\{\varnothing, \Omega, B, B^c\right\}$. By measurability of $\mathcal{G}$, it is constant on $B,B^c$:
\begin{equation*}
  \begin{gathered}
    \P(A\mid\mathcal{G})(\omega)= \begin{cases}a\quad \omega\in B\\ b\quad\omega\in B^c\end{cases}
  \end{gathered}
\end{equation*}\par
\noindent we get:
\begin{equation*}
  \begin{gathered}
    a\P(B) = \int_B I_A d\P = \int_\Omega I_AI_Bd\P = \int_\Omega I_{A\cap B} d\P = \overbrace{\P(A\cap B)}^{=\P(A\mid B)}
  \end{gathered}
\end{equation*}\par
\noindent This tells us $a = \dfrac{\P(A\cap B)}{\P(B)}$. Similarly for $b = \dfrac{\P(A\cap B)}{\P(A)} = \P(B\mid A)$
\par\bigskip
\subsection{Independent random variables}\hfill\\
\par\bigskip
\noindent If $X_1,\cdots,X_n$ independent. $\E(h(X_1,\cdots,X_n))$ where $h$ is a function involving one or more $X_i$, what is $\E(h(X_1,\cdots,X_n)\mid X_1) = g(X_1)$ where $g = \E(h(x,X_2,\cdots,X_n))$.\par
\noindent This follows from Fubinis theroem, since $\mathcal{G} = \sigma(X_1)$, we only need to check that it satisfies conditions and events of the form $\left\{x_1\in A\right\}$ for $A\in\Sigma$
\begin{equation*}
  \begin{gathered}
    \int_{\left\{x_1\in A\right\}}h(X_1,\cdots,X_n)d\P
  \end{gathered}
\end{equation*}\par
\noindent Recall that $\P^1\times P^1$ and $\P(A_1\cap A_2) = \P(A_1)\P(A_2)$, so we can split up $\P$ into all components:
\begin{equation*}
  \begin{gathered}
    \int_\R I_{\left\{x_1\in A\right\}}\underbrace{\iint\cdots\int}_{\R^{n-1}}h(x_1,\cdots,X_n)d(\Lambda_2\times\cdots\times\Lambda_n)d\Lambda_1
  \end{gathered}
\end{equation*}\par
\noindent The idea is to express using laws and then by independence use Fubinis theorem to be able to use indicator functions
\begin{equation*}
  \begin{gathered}
    =\int_{\left\{x_1\in A\right\}}g(X_1)d\Lambda_1
  \end{gathered}
\end{equation*}\par
\noindent So 3) holds, and 1-2) are immediate.
\par\bigskip
\noindent\textbf{Example:}\par
\noindent Let $X_1,\cdots,X_n$ be independent random variables and $\overline{X} = \dfrac{1}{n}\sum X_i$. What is $\E(\overline{X}\mid X_1)$, we can plug $h = \overline{X}$ and $g = \overline{X}$ with fixing $X_1$ we get $\E(\overline{X}\mid X_1) = g(X_1) = \dfrac{1}{n}\E(x+X_2+\cdots+X_n)$, by linearity
\begin{equation*}
  \begin{gathered}
    \dfrac{x+\E(X_2)+\cdots+\E(X_n)}{n} = \dfrac{X_1}{n}+\dfrac{\sum \E(X_i)}{n}
  \end{gathered}
\end{equation*}
\par\bigskip
\section{Martingales}\par
\begin{defo}[Martingale]{}
  Let $X_1,\cdots,$ be a sequence of integrable random variables, we say that the sequence is a martingale if $\E(X_{n+1}\mid X_n,X_{n-1},\cdots) = X_n$ almost surely
\end{defo}
\par\bigskip
\noindent On average we have no change. The expectation on the $n+1$:th outcome given the knowledge of previous outcomes is the same as the last outcome.
\par\bigskip
\begin{theo}[Properties of martingales]{}
  \begin{enumerate}[leftmargin=*]
    \item $\E(\E(X\mid\mathcal{G})) = \E(X)$
    \item If $X$ is $\mathcal{G}$-measurable, then $\E(X\mid\mathcal{G}) = X$ almost surely
    \item Linearity condition still holds: $\E(aX+bY\mid \mathcal{G}) = a\E(X\mid\mathcal{G})+b\E(Y\mid\mathcal{G})$ almost surely
    \item Positivity: If $X\geq0$ almost surely, then $\E(X\mid\mathcal{G})\geq0$ almost surely irregardless of $\mathcal{G}$
  \end{enumerate}
\end{theo}
\par\bigskip
\begin{prf}[Properties of martingales]{}
  \begin{enumerate}[leftmargin=*]
    \item $\E(\E(X\mid\mathcal{G}))\Rightarrow \int_\Omega\E(X\mid\mathcal{G})d\P =\int_\Omega Xd\P$
    \item $X$ satisfies condition of conditional expectation (measurable on any subset and equality)
    \item Follows by linearity of $\int \cdots d\P$
    \item Suppose $Y = \E(X\mid\mathcal{G}) >0$, for positive measure set $A$. Then $\exists n$ such that $\P(Y\leq\dfrac{-1}{n})>0$. This is a $\mathcal{G}$-measurable set and $\int_{A_n}Yd\P \leq \dfrac{-1}{n}\underbrace{\P(A_n)}_{\geq0} = \int_{A_n}X d\P\geq0$ which is a contradiction.
  \end{enumerate}
\end{prf}
\par\bigskip
\noindent Of course, the results for integration are extended into results in expectation:
\par\bigskip
\begin{theo}[]{}
  \begin{enumerate}[leftmargin=*]
    \item If $X_1,\cdots$ are a sequence of non-negative random variables and $X_n\to X$, then $\E(X_n\mid\mathcal{G})\to \E(X\mid\mathcal{G})$ (MCT)
    \item If $X_1,\cdots$ satisfy $\left|X_i\right|\leq Z$ (where $Z$ is integrable and positive and $X_n\to X$), then $\E(X_n\mid\mathcal{G})\to\mathcal \E(X\mid\mathcal{G})$ (DCT)
    \item Fatous: If $X_1,\cdots$ are non-negative, then we can also take the $\lim_{n\to\infty}\inf$ out:
      \begin{equation*}
        \begin{gathered}
          \E(\lim_{n\to\infty}\inf X_n\mid\mathcal{G})\leq\lim_{n\to\infty}\inf\E(X_n\mid\mathcal{G})
        \end{gathered}
      \end{equation*}
  \end{enumerate}
\end{theo}
\par\bigskip
\noindent We get an analogy of Jensens inequality:
\par\bigskip
\begin{theo}[Jensens inequality for martingales]{}
  Let $g:I\to\R$ be convex on $I\subseteq \R$ and assume $X:\Omega\to I$ is integrable (as well as $g(X)$), then:
  \begin{equation*}
    \begin{gathered}
      \E(g(X)\mid\mathcal{G}) \geq g(\E(X\mid\mathcal{G}))
    \end{gathered}
  \end{equation*}\par
  \noindent Almost surely 
\end{theo}\par
\noindent Dealing with random variables we must remember the 0 probability set can produce strange results, hence the "almost surely" remark when dealing with random variables.
\par\bigskip
\noindent\textbf{Simplification rules}:\par
\begin{enumerate}[leftmargin=*]
  \item $\E(\E(X\mid\mathcal{G})\mid\mathcal{H})$ where $\mathcal{H}\subseteq\mathcal{G}$
  \item $\E(ZX\mid\mathcal{G}) = Z\E(X\mid\mathcal{G})$ if $Z$ is $\mathcal{G}$ measurable
  \item $\E(X\mid\sigma(\mathcal{G},\mathcal{H})) = \E(X\mid\mathcal{G})$ is $\mathcal{H}$ is independent of $\mathcal{G}$ and $X$
\end{enumerate}
\par\bigskip
\noindent\textbf{Special cases:}\par
\begin{itemize}
  \item $\E(X\mid\mathcal{G}) = \E(X)$ if $X$ is independent of $\mathcal{G}$
  \item $\E(X\mid\mathcal{G}) = X$ if $X$ is $\mathcal{G}$-measurable
  \item $\E(\E(X\mid\mathcal{G})) = \E(X)$
\end{itemize}\par
\noindent The proofs of this follows by checking the constraints
\par\bigskip
\noindent The previous paragraphs were a "basic intro" to martingales, lets delve into the deeper and a bit more rigourous definitions.
\par\bigskip
\subsection{Stochastic Process}\hfill\\
\par\bigskip
\begin{defo}[Discrete Stochastic Process]{}
  A sequence of random variables $X_0,X_1,\cdots$ is called a \textit{discrete random proces}
\end{defo}
\par\bigskip
\begin{defo}[Filtration]{}
  A \textit{filtration} is a sequence of $\sigma$-algebras $\mathcal{F}_0\subseteq\mathcal{F}_1\subseteq\cdots\subseteq\mathcal{F}$. We write $\mathcal{F}_\infty = \sigma\left(\bigcup_{i=0}^{\infty}\mathcal{F_i}\right)\subseteq\mathcal{F}$
\end{defo}
\par\bigskip
\begin{defo}[Adapted Process]{}
  We say $(X_n)$ is adapted to the filtration $(\mathcal{F}_i)$ if $X_n$ is $\mathcal{F}_n$-measurable $\forall n$
\end{defo}
\par\bigskip
\begin{defo}[Martingale]{}
  A martingale is a stochastic process adapted to a filtration $(\mathcal{F}_n)$ such that
  \begin{equation*}
    \begin{gathered}
      \E(X_n\mid\mathcal{F_{n-1}}) = X_{n-1}
    \end{gathered}
  \end{equation*}
  \par\bigskip
  \noindent Equivalently, we can express in increments:
  \begin{equation*}
    \begin{gathered}
      \E(X_{n}-X_{n-1}\mid\mathcal{F}_{n-1}) = \E(X_n\mid\mathcal{F}_{n-1})-\underbrace{\E(X_{n-1}\mid\mathcal{F}_{n-1})}_{=X_{n-1}} = 0
    \end{gathered}
  \end{equation*}
\end{defo}
\par\bigskip
\begin{defo}[Super-martingale]{}
  A super-martingale is a martingale such that $\E(X_n\mid\mathcal{F}_{n-1})\leq X_{n-1}$
\end{defo}
\par\bigskip
\begin{defo}[Sub-martingale]{}
  Is a martingale such that $\E(X_n\mid\mathcal{F}_{n-1})\geq X_{n-1}$
\end{defo}
\par\bigskip
\noindent\textbf{Example:}\par
\noindent Consider the standard random walk on $\Z$. We move with probablity $\dfrac{1}{2}$ either to the left or to the right. \par
\noindent Let $Y_1,\cdots, $ be iidrv wiht $\P(Y_i = 1) = \P(Y_i=-1) = \dfrac{1}{2}$\par
\noindent $X_0 =0$, $X_n = \sum_{i}^{n}Y_i = X_{n-1}+Y_n$\par
\noindent Let $\mathcal{F}_n = \sigma(X_0,X_1,\cdots,X_n)$. Then $(X_n)$ is adapted to $(\mathcal{F}_n)$ (measurable with respect to the $\sigma$-algebra, but it contains $(X_n)$)\par
\noindent Is it a martingale?
\begin{equation*}
  \begin{gathered}
    \E(X_n\mid\mathcal{F}_{n-1}) = \E(X_{n-1}+Y_n\mid\mathcal{F}_{n-1}) = \underbrace{\E(X_{n-1}\mid\mathcal{F}_{n-1})}_{\mathcal{F}_{n-1}\text{-measurable}}+\underbrace{\E(Y_n\mid\mathcal{F}_{n-1})}_{\text{indep.}} = \E(Y_n)\\
    = X_{n-1}\cdot1\dfrac{1}{2}1\cdot\dfrac{1}{2} = 0+X_{n-1}
  \end{gathered}
\end{equation*}\par
\noindent Hence a martingale. Note, no need of iid, only independence is required!
\par\bigskip
\noindent\textbf{Example:}\par
\noindent Let $Y_1,\cdots$ be independent random variables with $\E(Y_i) = 1$. Let $X_0 = 1$ and $X_n = X_0\cdot\prod_{k=1}^{n}Y_k$. Again, $X_n$ is adapted to $(\mathcal{F}_n)$ where $\mathcal{F}_n = \sigma(X_0,X_1,\cdots,X_n)$.\par
\noindent We check the martingale condition:
\begin{equation*}
  \begin{gathered}
    \E(X_n\mid\mathcal{F}_{n-1}) = \E(X_0\prod_{k=1}^{n}Y_k\mid\mathcal{F}_{n-1}) = \E(X_{n-1}Y_n\mid\mathcal{F}_{n-1}) = X_{n-1}\E(Y_n\mid\mathcal{F}_{n-1}) = X_{n-1}\E(Y_n) = X_{n-1}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Example:}\par
\noindent Let $X$ be a $\mathcal{F}$-measruable random variable. Let $\mathcal{F}_1,\cdots$ be a filtration. Then the conditional expecation with respect to previous filtration is still a martingale.\par
\noindent Let $X_n = \E(X\mid \mathcal{F}_n)$, $\E(X_n\mid\mathcal{F}_{n-1}) = \E(\E(X\mid\mathcal{F}_n)\mid\mathcal{F}_{n-1})$ where $\mathcal{F}_{n-1}$ is a coarser $\sigma$-algebra $= \E(X\mid\mathcal{F}_{n-1}) = X_{n-1}$
\par\bigskip
\noindent\textbf{Remark:}\par
\noindent Let $m<n$. For every martingale, we have:
\begin{equation*}
  \begin{gathered}
    \E(X_n\mid\mathcal{F}_m) = \E(\E(\cdots\E(\underbrace{\E(X_n\mid\mathcal{F_{n-1}})}_{=X_{n-1}}\mid\mathcal{F}_{n-2})\cdots)\mid\mathcal{F}_m)\\
    =\E(\E(\cdots\underbrace{\E(X_{n-1}\mid\mathcal{F}_{n-2})}_{X_{n-2}}\cdots\mid\mathcal{F}_{m+1})\mid\mathcal{F}_m)\\
    \vdots\\
    =\E(X_{m+1}\mid\mathcal{F}_m) = X_m
  \end{gathered}
\end{equation*}
\par\bigskip
\begin{defo}[Pre-visible process]{}
  A pre-visible process is a seuqnce $C_1,\cdots$ of random variables such that $C_n$ is $\mathcal{F}_{n-1}$-measurable $\forall n$
\end{defo}
\par\bigskip
\noindent Let $C_n$ be a pre-visible process. The martingale transform of $X$ by $C$ is 
\begin{equation*}
  \begin{gathered}
    (C\cdot X)_n = \sum_{k=1}^{n}C_k(X_k-X_{k-1})
  \end{gathered}
\end{equation*}\par
\noindent In particular, if $C_k = 1$ $\forall k$, then
\begin{equation*}
  \begin{gathered}
    (C\cdot X)_n = X_n-X_0
  \end{gathered}
\end{equation*}
\par\bigskip
\begin{defo}[]{}
  If $C$ is a bounded pre-visible process with $\left|C_n(\omega)\right|\leq K$ for all $n$ and $\omega\in\Omega$, then $(C\cdot X)_n$ is a martingale if $X_n$ is a martingale.
  \par\bigskip
  \noindent If $C$ is also non-negative, then $(C\cdot X)_n$ is a sub/super-martingale whenever $X_n$ is.
\end{defo}
\par\bigskip
\begin{prf}[]{}
  We have 
  \begin{equation*}
    \begin{gathered}
      \E((C\cdot X)_n-(C\cdot X)_{n-1}\mid\mathcal{F}_{n-1})\\
      = \E(C_n(X_n-X_{n-1})\mid\mathcal{F}_{n-1})\\
      = C_n(\E(X_n\mid\mathcal{F}_{n-1})-\E(X_{n-1}\mid\mathcal{F}_{n-1}))\\
      = C_n(\E(X_n\mid\mathcal{F}_{n-1})-X_{n-1}) = \begin{cases}=0\quad\text{if $X_n$ is a martingale}\\\geq0\quad\text{ if $C_n\geq0$ and $X_n$ is a sub-martingale}\\\leq0\quad\text{ if $C_n\geq0$ and $X_n$ is a super-martingale}\end{cases}
    \end{gathered}
  \end{equation*}
\end{prf}
