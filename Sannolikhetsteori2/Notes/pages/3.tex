\section{Multivariate random variables}
\par\bigskip
\noindent It is strongly suggested to recall the random $n$-dimensional vector from Probability theory 1.
\par\bigskip
\noindent It is interesting to look at that the distribution of the vector, but we are often interested in a function $g(X)$
\par\bigskip
\noindent\textbf{Example:}\par
\noindent Starting with 1-dimension, and then working our way up.\par
\noindent Let $Y = g(X)$. Suppose $g$ is strictly increasing of $X$ (larger values of $X\rightarrow g(X)$ is larger).
\par\bigskip
\noindent Then
\begin{equation*}
  \begin{gathered}
    Y\leq y\in\R\Lrarr g(X)\leq y\in\R\Lrarr X\leq g^{-1}(y) = h(y)
  \end{gathered}
\end{equation*}\par
\noindent If we look at the distribution function (which gives us everything, the probability the everything):
\begin{equation*}
  \begin{gathered}
    F_Y(y) = P(X\leq g^{-1}(y)) = P(X\leq h(y)) = F_X(h(y))
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Anm채rkning:}\par
\noindent The inverse function $g^{-1}$ is denoted by $h$
\par\bigskip
\noindent From the chain rule for derivates we get:
\begin{equation*}
  \begin{gathered}
    f_Y(y) = f_X(h(y))\cdot h^{\prime}(y)
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent We have gotten some information about $X$ from $Y$. We can of course do the same for strictly decreasing functions:
\begin{equation*}
  \begin{gathered}
    Y\leq y\Lrarr X\geq h(y)\\
    \Rightarrow F_Y(y) = P(X\geq h(y)) = 1-F_X(h(y))
  \end{gathered}
\end{equation*}\par
\noindent This is good since we know that density functions are always positive. Taking the derivative gives us:
\begin{equation*}
  \begin{gathered}
    f_Y(y) = -f_X(h(y))\cdot h^{\prime}(y)
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Anm채rkning:}\par
\noindent We assume $g$ is differentiable with an inverse.
\par\bigskip
\noindent We showed that $f_Y(y) = f_X(h(y))\cdot \left|h^{\prime}(y)\right|$
\par\bigskip
\subsection{Multivariate case}\hfill\\\par
\noindent Think of two $n$-dimensional space. One for $X = (X_1,X_2,\cdots,X_n)$, and one for $Y= (Y_1,Y_2,\cdots,Y_n)$
\par\bigskip
\noindent Suppose $g$ is a bijective function such that $g$ and $g^{-1}$ are differentiable and let $Y = g(X) = (g_1(X),g_2(X),\cdots, g_n(X))$ where the component $Y_i = g_i(X = (X_1,X$
\par\bigskip
\noindent We have $X = g^{-1}(Y) = h(Y)$ (same as in 1-dimensional case)
\par\bigskip
\begin{theo}[Transformation theorem]{}
  The density of $Y$ is given by
  \begin{equation*}
    \begin{gathered}
      f_Y(y_1,y_2,\cdots,y_n) = f_X(h_1(y),h_2(y),\cdots,h_n(y))\cdot\left|J\right|
    \end{gathered}
  \end{equation*}\par
  \noindent Where $J$ is the Jacobian matrix
  \begin{equation*}
    \begin{gathered}
      J = \left|\dfrac{d(x)}{d(y)}\right| = \begin{vmatrix}\dfrac{dx_1}{dy_1}&\cdots&\dfrac{dx_1}{y_n}\\\vdots&\vdots&\vdots\\\dfrac{dx_n}{dy_1}&\cdots&\dfrac{dx_n}{dy_n}\end{vmatrix}
    \end{gathered}
  \end{equation*}
\end{theo}
\par\bigskip
\noindent\textbf{Anm채rkning:}\par
\noindent Transformation theorem corresponds to multivariate analysis change of variables
\par\bigskip
\begin{prf}[Sketch of Transformation theorem]{}
  Let $y_0$ be a point in the $Y$-space. Choose an $\varepsilon$-ball $C$ around $y_0$. Then we can assume that $f_Y$ is constant in $C$.
  \par\bigskip
  \noindent The probability that our random vector $Y$ will happen in this region is given by
  \begin{equation*}
    \begin{gathered}
      \Delta C\cdot(f_Y(y_0)-\varepsilon)\leq P(Y\in C)\leq \Delta C\cdot(f_Y(y_0)+\varepsilon)
    \end{gathered}
  \end{equation*}
  \par\bigskip
  \noindent\textbf{Anm채rkning:} $\Delta C$ is the volume/area of $C$
  \par\bigskip
  \noindent In the $X$-space, there then is a region which consists of all $x$ whose $g(x)$ belongs to $C$.\par
  \noindent Since $g$ is bijective $\Rightarrow$ injective, we have that $Y\in C\Lrarr X\in D = g^{-1}(C)$
  \par\bigskip
  \noindent This means that these probabilities are the same
  \begin{equation*}
    \begin{gathered}
      \left|f_Y(y_0)\cdot\Delta C-\int_D f_X(x)dx\right|\leq\Delta C\cdot\varepsilon
    \end{gathered}
  \end{equation*}
  \par\bigskip
  \noindent As $C$ decreases, $\Delta C$ decreases as well as $\varepsilon$\par
  \noindent Since $g$ is a nice function, $D$ will also decreases\par
  \noindent We let $x_0 = g^{-1}(y_0) = h(y_0)$. We can replace the integral by $f_X(x_0)\cdot\Delta D$ and obtain
  \begin{equation*}
    \begin{gathered}
      f_Y(y_0)\cdot\Delta C\approx f_X(x_0)\cdot\Delta D\Lrarr f_Y(y_0)\approx f_X(x_0)\dfrac{\Delta D}{\Delta C}
    \end{gathered}
  \end{equation*}
  \par\bigskip
  \noindent We get equality when $C\to0$ (choosing a smaller and smaller $\varepsilon$)
  \par\bigskip
  \noindent Recall the functional determinant (Jacobian) of the matrix $\dfrac{\Delta x}{\Delta y} = \left|\dfrac{d(x)}{d(y)}\right|$ (relative volume change)
  \par\bigskip
  \noindent Thus, we get $f_Y(y_0) = f_X(h(y_0))\left|\left|J_n(x_0,y_0)\right|\right|$\par
  \noindent Since this is true for all $y$, we can take away the index $y_0$, and we get:
  \begin{equation*}
    \begin{gathered}
      f_Y(y) = f_X(h(y))\cdot\left|J\right|
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\noindent\textbf{Example (1-dim case):}\par
\noindent Suppose $g(X)=aX+b$. From this it is easy to see what the inverse function $h = g^{-1}$ is, namely $h(y) = \dfrac{y-b}{a}$.
\par\bigskip
\noindent The Jacobian is just $\dfrac{1}{a}$. By the transformation theorem we get
\begin{equation*}
  \begin{gathered}
    f_Y(y) = f_X\left(\dfrac{y-b}{a}\right)\cdot\left|\dfrac{1}{a}\right|
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent The main thing is that using the density function of $X$ we can get the density function for $Y$.
\par\bigskip
\noindent\textbf{Example 2.4:}\par
\textit{$X,Y$ are independent normally distributed random variables $N(0,1)$, show that $X+Y$ and $X-Y$ are independent and determine their distribution function.}
\par\bigskip
\noindent In order to solve this we do a variable substitution. Let $U = X+Y$ and $V = X-Y$.\par
\noindent Notice that $\dfrac{U+V}{2} = X$ and $\dfrac{U-V}{2} = Y$\par
\noindent We have our function $g(x,y) = (u,v) = (x+y,x-y)$\par
\noindent We have our inverse $g^{-1}(u,v) = (x,y) = \left(\dfrac{u+v}{2}, \dfrac{u-v}{2}\right)$\par
\noindent We can now use the transformation theorem:
\begin{equation*}
  \begin{gathered}
    f_{U,V}(u,v)=f_{X,Y}\left(\dfrac{u+v}{2}, \dfrac{u-v}{2}\right)\cdot\left|J\right|
  \end{gathered}
\end{equation*}\par
\noindent The Jacobian can be found:
\begin{equation*}
  \begin{gathered}
    J = \begin{vmatrix}\dfrac{1}{2}&\dfrac{1}{2}\\\\\dfrac{1}{2}&-\dfrac{1}{2}\end{vmatrix} = \dfrac{-1}{4}-\dfrac{1}{4} = \dfrac{-1}{2}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent By the transformation theorem:
\begin{equation*}
  \begin{gathered}
    f_{U,V}(u,v) = f_{X,Y}\left(\dfrac{u+v}{2}, \dfrac{u-v}{2}\right)\cdot\dfrac{1}{2}\stackrel{\text{indep.}}{=}f_X\left(\dfrac{u+v}{2}\right)f_Y\left(\dfrac{u-v}{2}\right)\cdot\dfrac{1}{2}
  \end{gathered}
\end{equation*}\par
\noindent We know their density functions since they are normally disitrbuted with $N(0,1)$:
\begin{equation*}
  \begin{gathered}
    = \dfrac{1}{\sqrt{2\pi}}e^{-(1/2)\left(\dfrac{u+v}{2}\right)^2}\cdot\dfrac{1}{\sqrt{2\pi}}e^{-(1/2)\left(\dfrac{u-v}{2}\right)^2}\cdot\dfrac{1}{2}\\
  \end{gathered}
\end{equation*}\par
\noindent After simplification we get that $f_{U,V}$ is a product of one function of $U$ and one function of $V$. This means that $U,V$ are independent since we get them as a product of two different functions. 
\par\bigskip
\noindent\textbf{Example:}\par
\noindent Recall the convolution formula from Probability theory 1; if $X,Y$ are independent, we have
\begin{equation*}
  \begin{gathered}
    f_{X+Y}(z) = \int_{-\infty}^\infty f_X(x)f_Y(z-x)dx
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent We can show this from the transformation theorem.
\par\bigskip
\noindent By the independance of $X,Y$, we have $f_{X,Y}(x,y) = f_X(x)f_Y(y) \forall x,y$\par
Let $Z = X+Y$. Then $g(X,Y) = (X+Y,X)$\par
The inverse is given by $Y = Z-X$. We can now use the transformation theorem:
\begin{equation*}
  \begin{gathered}
    f_{Z,Y}(z,x) = f_{X,Y}(h_1(z,x), h_2(z,x))\cdot\left|J\right|
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent The Jacobian is given by
\begin{equation*}
  \begin{gathered}
    \begin{vmatrix}1&-1\\1&0\end{vmatrix} = 1
  \end{gathered}
\end{equation*}\par
\noindent Since $X,Y$ are independent, we get:
\begin{equation*}
  \begin{gathered}
    f_{Z,X}(z,x) = f_Y(z-x)f_X(x)
  \end{gathered}
\end{equation*}\par
\noindent The marginal density is given by integrating away $x$:
\begin{equation*}
  \begin{gathered}
    f_Z(z) = \int_{-\infty}^{\infty}f_{Z,X}(z,x)dx = \int_{-\infty}^{\infty}f_X(x)f_Y(z-x)
  \end{gathered}
\end{equation*}
\par\bigskip
\subsection{Conditional Probabilities}\hfill\\
\par\bigskip
\noindent It is suggested to do some examples from Probability theory 1.
\par\bigskip
\noindent Let $X,Y$ be some random variables with joint discrete distribution (\textbf{TODO:}\textit{ Def}).\par
\noindent We look at the conditional distribution:
\begin{equation*}
  \begin{gathered}
    p_{Y|X=x}(y) = P(Y=y|X=x) = \dfrac{P_{X,Y}(x,y)}{P_X(x)}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent If we look at the conditional probability distribution function, we have: 
\begin{equation*}
  \begin{gathered}
    F_{Y|XX=x}(y) = \sum_{z\leq y}P_{Y|X =x}(z)
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent If we now look at if $X,Y$ have a joint continuous distribution, we have something similar, with a couple of things changed where we use the density function instead (since some probabilities are 0)
\begin{equation*}
  \begin{gathered}
    f_{Y|X=x}(y) = \dfrac{f_{X,Y}(x,y)}{f_{X}(x)}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Does this make sense for the continuous case? Well, suppose that $f_{Y|X=x_0}(y) = f_{X,Y}(x_0,y)$, would this be a natural definition?\par
\noindent The reason we cannot use this definition is because the probability that $X = x_0$ is very small because of continuous. But now we have this given, we have assumed this has happened, so $f_{X,Y}(x_0,y)$ could very well be too small compared to $f_{Y|X = x_0}(y)$ where we already know that $X=x_0$ happpens.\par
\noindent If we instead put $f_{Y|X=x_0}(y) = Kf_{X,y}(x_0,y)$ some constant to compensate, then we need to check if the properties for density functions are preserved:
\begin{equation*}
  \begin{gathered}
    \int_{-\infty}^{\infty}f_{Y|X=x_0}(y)dy = 1\Lrarr\text{ proper density function}
  \end{gathered}
\end{equation*}\par
\noindent However, with the $K$ in front, we get:
\begin{equation*}
  \begin{gathered}
    \int_{-\infty}^{\infty}f_{Y|X=x_0}(y)dy = 1 = \int_{-\infty}^{\infty}Kf_{X,Y}(x_0,y)dy\\
    = Kf_{X}(x_0) = 1\\
    \Rightarrow K = \dfrac{1}{f_X(x_0)}
  \end{gathered}
\end{equation*}\par
\noindent This is true for all $X =x_0$, so the formula we have seems to be correct!
\par\bigskip
\noindent Just as in the discrete case, we have:
\begin{equation*}
  \begin{gathered}
    F_{Y|X=x}(y) = \int_{-\infty}^{y}f_{Y|X=x}(z)dz
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Now we have all the tools to start define conditional expectations and conditional variances.
\newpage
\subsection{Conditional expected values and variances}\hfill\\\par
\noindent There are some natural definitions
\par\bigskip
\begin{theo}[Conditional Expected Value]{}
  In the continuous case we have:\par
  \begin{equation*}
    \begin{gathered}
      \E(Y|X=x) = \int_{-\infty}^{\infty}yf_{Y|X=x}(y)dy
    \end{gathered}
  \end{equation*}
  \noindent In the discrete case we have:\par
  \begin{equation*}
    \begin{gathered}
      \E(Y|X=x) = \sum_{-\infty}^{\infty}P_{Y|X=x}(y)
    \end{gathered}
  \end{equation*}
\end{theo}
\par\bigskip
\noindent Notice that $\E(Y|X=x)$ is a function of $x$.\par
\noindent We now look at only the random variable $\E(Y|X) = g(X)$ 
\par\bigskip
\noindent Sometimes it is easier to look at the conditional expected value.
\par\bigskip
\begin{theo}[]{}
  $\E(Y|X)$ has the same expected value as $\E(Y)$:\par
  \begin{equation*}
    \begin{gathered}
      \E(\E(Y|X)) = \E(g(X)) = \E(Y)
    \end{gathered}
  \end{equation*}
  \par\bigskip
  \noindent In the discrete case, we see a variant of the law of total probabilities:\par
  \begin{equation*}
    \begin{gathered}
      \E(Y) = \E(Y|X) = \sum_x \E(Y|X=x)P(X=x)
    \end{gathered}
  \end{equation*}
\end{theo}
\par\bigskip
\noindent Rules for caluclations, as in the unconditional case, can be found in the book (\textbf{TODO:}\textit{ Ins}) 
\begin{equation*}
  \begin{gathered}
    \E(f(X)Y|X) = f(X)\E(Y|X)
  \end{gathered}
\end{equation*}\par
\noindent Another natural rule is if $X,Y$ are independent:
\begin{equation*}
  \begin{gathered}
    \E(Y|X) =\E(Y)
  \end{gathered}
\end{equation*}
\par\bigskip
\begin{theo}[Conditional variance]{}
  \begin{equation*}
    \begin{gathered}
      v(x) = Var(Y|X = x) = \E((Y-\E(Y|X=x))^2|X=x)\\
      V(X) = Var(Y|X) = \E((Y-\E(Y|X))^2|X)
    \end{gathered}
  \end{equation*}
\end{theo}
\par\bigskip
\begin{theo}[]{}
  We define $e(X) = \E(Y|X)$ and $V(X) = Var(Y|X)$ and assume $g(X)$ is some function on $X$.\par
  \noindent Then we have:\par
  \begin{equation*}
    \begin{gathered}
      \E((Y-g(X))^2) = \E(V(X)) + \E((e(X)-g(X))^2)
    \end{gathered}
  \end{equation*}
\end{theo}
\pagebreak
\begin{prf}[]{}
  \begin{equation*}
    \begin{gathered}
      \E((Y-g(X))^2) = \E((Y-e(X)+e(X)-g(X))^2)\\
      = \E((Y-e(X))^2) +2 \E(Y-e(X))\E(e(X)-g(X)) +\E((e(X)-g(X))^2)\\
      = \E(\E((Y-e(X))^2)|X)+ 2\E(\E(Y-e(X)\E(e(X)-g(X)))|X)+ \E((e(X)-g(X))^2)\\
      =\E(v(X))+2(e(X)-g(X))\E(Y-e(X)|X)+\E((e(X)-g(x))^2)\\
      = \E(v(X))+2(e(X)-g(X))\underbrace{(e(X)-e(X))}_{\text{$=0$}}+\E((e(X)-g(x))^2)
    \end{gathered}
  \end{equation*}
  \par\bigskip
  \noindent In the middle term we used: 
  \begin{equation*}
    \begin{gathered}
      \E(\E(Y-e(X))|X) = \E(e(X)-e(X)|X) = e(X)-e(X)=0
    \end{gathered}
  \end{equation*}
\end{prf}
\par\bigskip
\noindent There is a nice corollary  that follows from this:
\par\bigskip
\begin{theo}[]{}
  \begin{equation*}
    \begin{gathered}
      Var(Y) = \E(Var(Y|X)) + Var(\E(Y|X))
    \end{gathered}
  \end{equation*}
\end{theo}
\par\bigskip
\noindent The proof of the corollary follows from Proof 3.2 by choosing $g(X) = \E(Y)$.\par
\noindent Then the last theorem gives the result directly:
\begin{equation*}
  \begin{gathered}
    \E((Y-e(X))^2) = \E(v(X))+\E((e(X)-e(Y))^2)
  \end{gathered}
\end{equation*}\par
\noindent But $e(X)$ has the same expected value as $Y$, as in $\E(Y) = \E(e(X))$, so we get:
\begin{equation*}
  \begin{gathered}
    \E(v(X))+\E((e(X)-e(Y))^2) = \E(v(X))+Var(\E(Y|X))
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Recall that ranom variables do not only have values attached to them, but also parameters. For example $X\sim N(\mu,\sigma^2)$ where $\mu,\sigma^2$ are parameters.
\par\bigskip
\noindent For example, if $X\sim N(\mu,\sigma^2)$ and $X =x$ is an outcome of this random variable, then both $x,\mu\in\R$ while $X$ is a random variable.
\par\bigskip
\noindent Sometimes we want to think of the parameters as random variables.
\par\bigskip
\noindent\textbf{Example:}\par
\noindent Suppose we go to a hospital to take a blood-test and count the number of red blood cells in the sample, then we will get some value which also partly depends on some randomness.\par
\noindent First we look at some individual (even if I go to the hopsital several times, each time will give different results).
\par\bigskip
\noindent This seemingly random variation from the same person can be explained by the Poisson-distribution with some parameter $m$.\par
\noindent This means that if $X$ is an observed value, $X\sim Po(m)$. The value $m$ can be different across people. 
\par\bigskip
\noindent We can think that we do a random trial in 2 steps:\par
\begin{itemize}
  \item Choose a random individual to take the blood test from
  \item Count the amount of red blood cells from that individual
\end{itemize}\par
\noindent Then we can let $X$ be the observed value for that person, and we have $X|M = m\sim Po(m)$ with $M$ having some distribution (does not need to have Poisson distribution)
\par\bigskip
\noindent\textbf{Example:}\par
\noindent By the law of total probability, we can look at:
\begin{equation*}
  \begin{gathered}
    P(A) = \int_{-\infty}^{\infty}P(A|M=x)f_M(x)dx
  \end{gathered}
\end{equation*}\par
\noindent Suppose now that $M\sim Exp(1)$. We then get:
\begin{equation*}
  \begin{gathered}
    P(X=k)= \int_{-\infty}^{\infty}P(X=k|M=x)f_M(x)dx\\
    =\int_{0}^{\infty}e^{-x}\dfrac{x^k}{k!}e^{-x}dx=\dfrac{1}{k!}\int_{0}^{\infty}x^ke^{-2x}dx\\
    \dfrac{1}{k!}\int_{0}^{\infty}\dfrac{y^k}{2^k}e^{-y}\dfrac{1}{2}dy=\dfrac{1}{k!+2^{k+1}}\int_{0}^{\infty}y^ke^{-y}dy\\
    \Rightarrow k!\Rightarrow P(X=k) = \dfrac{1}{2^{k+1}}
  \end{gathered}
\end{equation*}\par
\noindent So $X$ has a geometric distribution. One can also get this from the $\Gamma$ distribution since it is very similar:
\begin{equation*}
  \begin{gathered}
    \Gamma(z) = \int_{0}^{\infty}x^{z-1}e^{-x}dx 
  \end{gathered}
\end{equation*}\par
\noindent When $z = k$ then $\Gamma(z) = (k-1)!$
\par\bigskip
\noindent In the last example, we do not need to calculate the unconditional distribution of $X$ if we just want to know the expected value/variance of $X$ using the formulas that we proved above.
\par\bigskip
\noindent Recall that we can write $\E(X) = \E(\E(X|M))$. We said that $X|M\sim Po(m)$, and we know that a Poisson random variable has $\E(X|M) = M$, and we get:
\begin{equation*}
  \begin{gathered}
    \E(X) = \E(\E(X|M)) = \E(m)
  \end{gathered}
\end{equation*}\par
\noindent Also remember that $M\sim Exp(1)$, so $\E(M) = 1$, and we have:
\begin{equation*}
  \begin{gathered}
    \E(X) = \E(\E(X|M)) = \E(M) = 1
  \end{gathered}
\end{equation*}\par
\noindent By the corollary, we can also find the variance:
\begin{equation*}
  \begin{gathered}
    Var(X) = \E(Var(X|M))+\underbrace{Var(\E(X)|M)}_{\text{$Var(X|M) = M$}} = \E(M)+\underbrace{Var(M)}_{\text{$=m=1$}} = 1+1 = 2
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Example:}\par
\noindent Suppose we are in a coffee shop, and every customer has a choice between coffee (with probability $p$) or tea (with probability $1-p$).\par
\noindent Suppose the number of customers during lunchtime $N$ is $\sim Po(\lambda)$ distributed. We want to count the number of coffees ordered in total (or rather, find the distribution of the number of coffees).
\par\bigskip
\noindent We proceed by letting $X$ be number of coffees ordered, and let $N$ be the number of customers. We actually know because of how we assumed it, we know the amount of customers, we have a binomial distribution:
\begin{equation*}
  \begin{gathered}
    X|N=n\sim Bin(n,p)
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent From our example we also know that $N\sim Po(\lambda)$, we have $P(X=k|N=n) = \begin{pmatrix}n\\k\end{pmatrix}p^kq^{n-k}$.\par
\noindent Now we want to count $P(X=k)$ without $N=n$. We can use the law of total probabilities: 
\begin{equation*}
  \begin{gathered}
    P(X=k) =\sum_{n=0}^{\infty}P(X=k|N=n)P(N=n)\\
    = \sum_{n=0}^{\infty}\begin{pmatrix}n\\k\end{pmatrix}p^kq^{n-k}e^{-\lambda}\dfrac{\lambda^n}{n!}\qquad \begin{pmatrix}n\\k\end{pmatrix} = \dfrac{n!}{k!(n-k)!}\\
    \Rightarrow \dfrac{p^k}{k!}e^{-\lambda}\sum_{n=0}^{\infty}\dfrac{\lambda^n}{(n-k)!}q^{n-k}\\
    = \dfrac{(\lambda p)^k}{k!}e^{-\lambda}\sum_{n=0}^{\infty}\dfrac{(\lambda q)^{n-k}}{(n-k)!}
  \end{gathered}
\end{equation*}\par
\noindent Let $j = n-k$, then we can rewrite the sum as:
\begin{equation*}
  \begin{gathered}
    \dfrac{(\lambda p)^k}{k!}e^{-\lambda}\underbrace{\sum_{j=0}^{\infty}\dfrac{(\lambda q)^j}{j!}}_{\text{$=e^{\lambda q}$}}\\
    \dfrac{(\lambda p)^k}{k!}e^{-\lambda}e^{\lambda q} = \dfrac{(\lambda p)^k}{k!}e^{-\lambda}e^{\lambda (1-p)} = \dfrac{(\lambda p)^k}{k!}e^{-\lambda p}
  \end{gathered}
\end{equation*}\par
\noindent This looks like a Poisson distribution, but $\lambda$ is changed to $\lambda p$. Therefore it is a Poisson distribution with parameter $\lambda p\Rightarrow X\sim Po(\lambda p)$
\par\bigskip
\section{Exercises}\par
\subsection{Exercise 1.5}\hfill\\
\textit{Show that if $X\sim C(0,1)$, then $X^2\sim F(1,1)$}
\par\bigskip
\textbf{Anm채rkning:}\par
\noindent $C$ is a Cauchy distribution, and $F$ is a Fisher distribution. Note also that $\Gamma(1) = 1$ and that $\Gamma\left(\dfrac{1}{2}\right)=\sqrt{\pi}$
\par\bigskip
\noindent Let $Y= X^2$. We want to show that it has distribution $\dfrac{1}{\pi}\dfrac{x^{-1/2}}{1+x}$. Taking the derivative of $F_Y(y) = P(Y\leq Y)$ yields the density function.\par
\noindent Want to show that this is $\dfrac{y^{-1/2}}{1+y}\dfrac{1}{\pi}$:
\begin{equation*}
  \begin{gathered}
    P(Y\leq y) = P(X^2\leq y) = P(\left|X\right|\leq \sqrt{y})\\
    = P(-\sqrt{y}\leq X \sqrt{y})
  \end{gathered}
\end{equation*}\par
\noindent From the density function of $X$, we can calculate this:
\begin{equation*}
  \begin{gathered}
    = \int_{-\sqrt{y}}^{\sqrt{y}}\dfrac{1}{\pi}\dfrac{1}{1+x^2}dx
  \end{gathered}
\end{equation*}\par
\noindent Since we want the derivative, we can use the fundamental theorem of calculus to take the derivative of the integral:
\begin{equation*}
  \begin{gathered}
    f_Y(y) = \dfrac{d}{dy}\dfrac{1}{\pi}\int_{-\sqrt{y}}^{\sqrt{y}}\dfrac{1}{1+x^2}dx\\
    \stackrel{\text{Sym.}}{=} 2\int_{0}^{\sqrt{y}}\dfrac{1}{1+x^2}dx \Rightarrow \dfrac{2}{\pi}\dfrac{1}{y}\cdot \dfrac{d}{dy}\sqrt{y} = \dfrac{2}{\pi}\dfrac{1}{1+y}y^{-0.5}\dfrac{1}{2}\\
    = \dfrac{1}{\pi}\dfrac{y^{-0.5}}{1+y} = F(1,1)
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Anm채rkning:}\par
\noindent One could have used the transformation theorem here to arrive at the same answer. 
\par\bigskip
\subsection{Exercise 1.13}\hfill\\
\textit{Let $X,Y$ have a joint density function given by}
\begin{equation*}
  \begin{gathered}
    f(x,y) = 
    \begin{cases*}
      1\qquad 0\leq x\leq2, \max(0,x-1)\leq y\min(1,x)\\
      0
    \end{cases*}
  \end{gathered}
\end{equation*}\par
\textit{determine the joint distribution functions and the joint and marginal distribution function}.
\par\bigskip
\noindent We have several different cases to check in this problem depending on where $(x,y)$ is situated. We will look at one trivial case and one non-trivial case.\par
In general, we know how to write out the distribution of a 2-dimensional r.v:
\begin{equation*}
  \begin{gathered}
    F(x,y)\int_{-\infty}^{x}\int_{-\infty}^{y}f(u,v)dudv
  \end{gathered}
\end{equation*}\par
\noindent In our case we have a uniform distribution, we can use this to skip a lot of the calculations. We integrate over the rectangle covered by $(x,y)$, then the trivial case is where the area we are interested in is covered by $(x,y)$ and the area of that area of interest is 1. The non-trivial part arises when we look at some point inside the area. Another trivial case is putting the point at $(0,0)$, then the area covered is 0.
\par\bigskip
In the non-trivial case where we assume $(x,y)$ is somewhere in the left part of the area. It is calculated to be  $xy-\dfrac{y^2}{2}$
\par\bigskip
From this, we can calculate the marginal distributions pretty easily by integrating away $x,y$
\par\bigskip
\subsection{Exercise 1.19}\hfill\\
\textit{The random vector $\mathbf{X} = (X_1,X_2,X_3)$ with density function}
\begin{equation*}
  \begin{gathered}
    f_{\mathbf{X}}(\mathbf{x}) = 
    \begin{cases*}
      \dfrac{2}{2e-5}x_1^2x_2e^{x_1x_2x_3}\qquad 0<x_1, x_2,x_3<1\\
      0
    \end{cases*}
  \end{gathered}
\end{equation*}\par
\textit{Determine the distribution of $X_1\cdot X_2\cdot X_3$}
\par\bigskip
\noindent We proceed using the transformation theorem. We define $y_1 = x_1x_2x_3$, $y_2 = x_1x_2$, $y_3 = x_1$. The reason we write it like this is because it is easier to find the inverse of these functions:
\begin{equation*}
  \begin{gathered}
    x_1 = y_3\\
    x_2 = \dfrac{y_2}{x_1} = \dfrac{y_2}{y_3}\\
    x_3 = \dfrac{y_1}{x_1x_2} = \dfrac{y_1}{y_3\dfrac{y_2}{y_3}} = \dfrac{y_1}{y_2}
  \end{gathered}
\end{equation*}\par
\noindent All the preparation is done, we use the transformation theorem:
\begin{equation*}
  \begin{gathered}
    f_Y(y) = f_X(x)\left|J\right|\\
    J = \begin{vmatrix}0&0&1\\0&\dfrac{1}{y^3}&-\dfrac{y_2}{y_3^2}\\\dfrac{1}{y_2}&-\dfrac{y_1}{y_2}&0\end{vmatrix} = \dfrac{-1}{y_2y_3}\Rightarrow \left|J\right| = \dfrac{1}{y_2y_3}
  \end{gathered}
\end{equation*}\par
\noindent Plug and chugg:
\begin{equation*}
  \begin{gathered}
    f_Y(y) = \dfrac{2}{2e-5}y_3^2\dfrac{y_2}{y_3}e^{y_1}\dfrac{1}{y_2y_3} = \dfrac{2}{2e-5}e^{y_1}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Since $y_1 = x_1x_2x_3$ and $x_1x_2x_3$ are all between $0,1$, therefore $y_1<y_2<y_3$.
\par\bigskip
\noindent We want to find $Y_1 = X_1X_2X_3$, but this is just the marginal distribution in $Y_1$ of $Y = (Y_1,Y_2,Y_3)$. By integrating away $Y_2, Y_3$ we can get this:
\begin{equation*}
  \begin{gathered}
    F_{Y_1}(y_1) = \int_{y_1}^{1}\int_{y_2}^{1}\dfrac{2}{2e-5}e^{y_1}dy_3dy_2\\
    = \dfrac{2}{2e-5}e^{y_1}\int_{y_2}^{1}(1-y_2)dy_2
  \end{gathered}
\end{equation*}\par
\noindent As an exercise, show that this integral is equal to $\dfrac{1}{2e-5}(1-y_1)^2e^{y_1}$ for $0<y_1<1$
