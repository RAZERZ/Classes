\section{Sample \& Random Matrices}
\par\bigskip
\subsection{Slide 3 - Expectation}\hfill\\
\par\bigskip
\noindent For a discrete random variable we use summation, for a continuous random variable we use integrals.\par
\noindent What do we use for vectors/matrices?
\par\bigskip
\noindent$\Rightarrow$ We perform the operations elementwise in the matrix. Take $\mathbb{E}(X_{ij})$ 
\par\bigskip
\subsection{Slide 4 - Covariance Matrix}\hfill\\
\par\bigskip
\noindent Recall 
\begin{equation}
  \begin{gathered}
     \text{Cov}\left(X,Y\right) = \E(X-\E(X)(Y-\E(Y))) = \E(XY)-\E(X)\E(Y)
  \end{gathered}
\end{equation}
\par\bigskip
\noindent for scalars.\par
\noindent What about $\text{Cov}\left(\begin{pmatrix}X_1\\X_2\\X_3\end{pmatrix}, \begin{pmatrix}Y_1\\Y_2\end{pmatrix}\right)$?
\par\bigskip
\noindent We can pick any pair $(X_i,Y_j)$ and compute $\text{Cov}\left(X_i,Y_j\right)$ leading to the same as (1) but with $X_i, Y_j$ instead.
\par\bigskip
\noindent In the case $\begin{pmatrix}X_1\\X_2\\X_3\end{pmatrix}, \begin{pmatrix}Y_1\\Y_2\end{pmatrix}$, we get a $3\times 2$ matrix where the $i,j$th elements corresponds to $\text{Cov}\left(X_i,Y_j\right)$.
\par\bigskip
\noindent Think of it like
\begin{equation}
  \begin{gathered}
    XY^T = \begin{pmatrix}X_1Y_1 &X_1Y_2\\X_2T_1&X_2Y_2\\X_3Y_1&X_3Y_2\end{pmatrix}
  \end{gathered}
\end{equation}
\par\bigskip
\noindent Now look at $\E(XY^T)$, same as (2) but $\E(X_iY_j)$.\par
\noindent Then we can easily see that $\text{Cov}\left(X,Y\right) = \E(XY^T)-\mu_X\mu_Y^T$
\par\bigskip
\noindent\textit{What if $X$ is continuous and $Y$ discrete?}\par
\noindent\textit{What if $Y=X$?}
\begin{equation*}
  \begin{gathered}
    \text{Cov}\left(X_i,X_i\right) = \E(X_i^2)-(\E(X))^2 = \text{Var}\left(X_i\right)
  \end{gathered}
\end{equation*}
\par\bigskip
\subsection{Slide 5 - Covariance Matrix}\hfill\\
\par\bigskip
\noindent Since in the scalar case $\text{Cov}\left(X_i,X_j\right) = \text{Cov}\left(X_j,X_i\right)$, then $\text{Cov}\left(X,Y\right) = \sum = $ symmetric \& positive definite.
\par\bigskip
\begin{theo}[Positive \& Semi-definite]{}
  Definite matrix $A$:
  \begin{equation*}
    \begin{gathered}
      A>0\Lrarr x^TAx>0
    \end{gathered}
  \end{equation*}
  \par\bigskip
  Semi-definite matrix $A$:
  \begin{equation*}
    \begin{gathered}
      A\geq0\Lrarr x^TAx\geq0
    \end{gathered}
  \end{equation*}
\end{theo}
\par\bigskip
\subsection{Slide 6 - Linear Combination}\hfill\\
\par\bigskip
\noindent You can view the vector $c$ as regression values for example
\par\bigskip
\subsection{Slide 7 - Linear Combination}\hfill\\
\par\bigskip
\noindent\textbf{Example}:
\begin{equation*}
  \begin{gathered}
    \text{Var}\left(X_1+2X_2+4X_3\right)\sim \text{Var}\left(\begin{pmatrix}1&2&4\end{pmatrix}\begin{pmatrix}X_1\\X_2\\X_3\end{pmatrix}\right)
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent A tip for remembering where to put $c^T$, think of it like matching dimensions of left hand side and right hand side.
\par\bigskip
\noindent We only want to compute expectation for the random stuff, so we can chuck coefficients and constants out.
\par\bigskip
\subsection{Slide 9 - Independence}\hfill\\
\par\bigskip
\noindent For simplicity, we define independence in the continuous case as $f(X,Y) = f(X)f(Y)$ and in the discrete case as $P(X,Y) = P(X)P(Y)$
\par\bigskip
\noindent\textbf{Anmärkning:} Jist because $\text{Cov}\left(X,Y\right)=0$ does not imply independence. Take the unit circle and the contour as pairs over $(X,Y)$. It is clear that $(X,Y)$ are dependant but their covariance is 0 since for every point on the circle you can reflect the $X,Y$ and therefore, by $\text{Cov}\left(X,Y\right) = \E(XY)-\E(X)\E(Y)$, you would be adding a bunch of 0. Same goes for any function that can be reflected.
\par\bigskip
\subsection{Slide 10 - Random Sample}\hfill\\
\par\bigskip
\noindent \textbf{Example} (Scalar case):\par
\noindent Let \textbf{x} $\sim$ \textbf{$x_1 x_2 x_3 \cdots$} be a random sample from $N(\mu,\sigma^2)$
\par\bigskip
\noindent We look at what it means for scalar random variables to be independent:
\begin{equation*}
  \begin{gathered}
    F(X,Y) = F(X)F(Y)\\
    f(x,y) = f(x)f(y)\\
    p(x,y) = p(x)p(y)
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent The same principle goes for random vectors, eg:
\begin{equation*}
  \begin{gathered}
    X_{n\times p} = \begin{pmatrix}x_1^T\\x_2^T\\\vdots\\x_n^T\end{pmatrix}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Think of each row as a sample from a different place $\Rightarrow$ independence in row $\Rightarrow$ random sample.
\par\bigskip
\noindent\textbf{Non-example:} Looking at the pulse of 1 person is not an independent response since it is only about 1 person. Even if you sampled a bunch of values from the same person into a matrix, that would still be a non-independent sample since we only sample from 1 person.
\par\bigskip
\noindent\textbf{Non-example:} Let us assume there is a competition between Uppsala and Lund in Multivariate Analysis. Everyone in the class at Uppsala has had the same teacher, so the values collected from that class are not independent.
\par\bigskip
\subsection{Slide 12 - Some Notes on Sample Covariance Matrix}\hfill\\
\par\bigskip
\noindent Unbiased becomes biased during non-linear \& non-affine transformations.
\par\bigskip
\noindent Even for large $n$, sometimes you cannot ignore the difference between $S_n$ and $S$ (eg. determining exact distributions)
\par\bigskip
\subsection{Slide 17 - Sample Covariance Matrix}\hfill\\
\begin{equation*}
  \begin{gathered}
    \begin{bmatrix}\mathbf{x}_1^{\prime}-\mathbf{\overline{x}}^{\prime}\\\mathbf{x}_2^{\prime}-\mathbf{\overline{x}}^{\prime}\\\vdots\\\mathbf{x}_n^{\prime}-\mathbf{\overline{x}}^{\prime}\end{bmatrix} = \underbrace{\mathbf{X}}_{n\times p}-\underbrace{\mathbf{1}}_{n\times1}\underbrace{\mathbf{\overline{x}}_1^{\prime}}_{1\times p}
  \end{gathered}
\end{equation*}
\begin{equation*}
  \begin{gathered}
    X-\dfrac{1}{n}\mathbf{1}\mathbf{1}^TX = (I-\dfrac{1}{n}\mathbf{1}\mathbf{1}^T)X
  \end{gathered}
\end{equation*}\par
\noindent So for $(X-\dfrac{1}{n}\mathbf{1}\mathbf{1}^TX)^T(X-\dfrac{1}{n}\mathbf{1}\mathbf{1}^TX)$:
\begin{equation*}
  \begin{gathered}
    X^T(I-\dfrac{1}{n}\mathbf{1}\mathbf{1}^T)^T(I-\dfrac{1}{n}\mathbf{1}\mathbf{1}^T)X = X^T\left[I-\dfrac{1}{n}\mathbf{1}\mathbf{1}^T-\dfrac{1}{n}\mathbf{1}\mathbf{1}^T-\dfrac{1}{n}\mathbf{1}\mathbf{1}^T+\dfrac{1}{n}\mathbf{1}\underbrace{\mathbf{1}^T\mathbf{1}}_{\text{$=n$}}\mathbf{1}^T\right]X\\
    X^T(I-\dfrac{1}{n}\mathbf{1}\mathbf{1}^T-\dfrac{1}{n}\mathbf{1}\mathbf{1}^T+\dfrac{1}{n}\mathbf{1}\mathbf{1}^T)X = X^T(I-\dfrac{1}{n}\mathbf{1}\mathbf{1}^T)X\\
    X^TX-X^T\mathbf{1}\mathbf{1}^TX\Rightarrow S_n = \dfrac{1}{n}\underbrace{X^TX}_{\text{Data matrix}}-(\dfrac{1}{n}X^T\mathbf{1})(\dfrac{1}{n}\mathbf{1}^TX)\\
    \text{Cov}\left(X\right) = \E(XX^T)^n-\E(X)\E(X)^T
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent\textbf{Anmärkning:}\par
\noindent\textbf{1} is an $n\times 1$ vector of ones.
