\section{Forts. Icke-linjära ekvationer}
\par\bigskip
\noindent Vi ska börja med att sammanfatta egenskaper hos bisektionsmetoden och Newton-Raphson:

\begin{center}
  \begin{tabular}{|c|c|}
    \hline
    Bisektion&Newton\\
    \hline
    Robust: Konvergerar alltid givet teckenväxling\\ och kontinuitet&Kan divergera, ej helt pålitlig\\
    \hline
    Linjär konvergens&Kvadratisk konvergens (snabb om startgissning bra)\\
    \hline
    1 funktionseval. per iteration&1 funktionseval. + 1 derivering per iteration\\
    \hline
  \end{tabular}
\end{center}
\par\bigskip
\noindent Vi noterar att där den ena är bra, så är den andra sämre. Det vore därför bra om man kan bygga en slags hybridmetod (många av dagens metoder ser  ut på detta vis). En sådan metod kan se ut på följande:

\subsection{Hybrid-skiss av Newton-Raphson och bisektionsmetoden}\hfill\\
\par\bigskip
\noindent Vi vill:
\begin{itemize}
  \item Kombinera säkerheten hos bisektionsmetoden med snabbheten hos Newtons metod 
\end{itemize}
\par\bigskip
\noindent Då behöver vi ett startintervall $[a,b]$ där vi vet teckenväxling finns och att det finns en rot, men det bästa är ju om vi kan köra Newton-Raphson eftersom den är snabb. Däremot kanske man hamnar utanför intervallet med Newton-Raphson, och det kan hända att antal iterationer blir för mycket, men \textit{om} den har konvergerat så är vi klara.\par\noindent Annars (ex. vis om vi hamnat utanför intervallet), då kör vi bisektionsmetoden för att krympa intervallet för att generera en bättre startgissning för Newton-Raphson och gör om algoritmen.
\par\bigskip
\noindent Något som kan hända är givet en funktion med flera rötter, så kan halvering av ett intervall "ta bort" rötter vilket kanske gör att (beroende på tillämpning) man får underliga svar. Exempel, om man mäter längden på en människa under sin livsstid där 150cm är roten, då kanske man passerar den när man växer, och senare i livet när man krymper. Halverar man däremot kanske man bara får vid tillväxten vilket kanske verkar konstigt givet att man vet att man krymper senare.
\par\bigskip
\subsection{Mer om konvergensordning i praktiken}\hfill\\
\par\bigskip
\noindent Vi påminner oss om definitionen:


\begin{equation*}
  \begin{gathered}
    \lim_{i\to\infty}\dfrac{\left|x_*-x_{i}\right|}{\left|x_*-x_{i-1}\right|^r} = C
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Där $r$ är konvergensordningen. Oftast är $x_*$ okänd, då tittar man istället på följande kvot:


\begin{equation*}
  \begin{gathered}
    \dfrac{\left|x_{i+1}-x_i\right|}{\left|x_i-x_{i-1}\right|^r} = \dfrac{\left|\Delta x_i\right|}{\left|\Delta x_{i-1}\right|^r}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Exempel:\par
\noindent En metod för ekvationslösning har generat korrektionstermerna ($\Delta x$):

\begin{center}
  \begin{tabular}{|c|c|}
    \hline
    $i$&$\Delta x_i$\\
    \hline
    1&0.01\\
    \hline
    2&0.001\\
    \hline
    3&$10^{-5}$\\
    \hline
    4&$10^{-9}$\\
    \hline
  \end{tabular}
\end{center}\par
\noindent Vad kan vi säga om metodens konvergens? Kan vi inte säga något? Är den linjär? Eller är den kvadratisk? Kubisk?\par
\noindent Vad är den asymptotiska felkonstanten?
\par\bigskip
\noindent Man kan besvara dessa frågor på lite olika sätt. Vi kan anta olika saker och se:

\begin{center}
  \begin{tabular}{|c|c|c|c|}
    \hline
    $i$&$\Delta x_i$&$\left|\Delta x_i\right|/\left|\Delta x_{i-1}\right|$&$\left|\Delta x_i\right|/\left|\Delta x_{i-1}\right|^2$\\
    \hline
    1&0.01&-&-\\
    \hline
    2&0.001&0.1&10\\
    \hline
    3&$10^{-5}$&0.01&10\\
    \hline
    4&$10^{-9}$&0.0001&10\\
    \hline
  \end{tabular}
\end{center}\par

\noindent Notera att tillväxten är kvadratisk eftersom skillnaderna mellan $\left|\Delta x_i\right|$ är kvadratisk, så om vi antar att den är kvadratisk (som vi gör i sista kolonnen) ser vi att den asymptotiska felkonstanten är 10.
\par\bigskip
\noindent Man kan använda 2 iterationer för att få fram $C$ och $r$ genom att lösa följande ekvationssystem:


\begin{equation*}
  \begin{gathered}
    \begin{cases*}
      \left|\Delta x_{i+1}\right| = C\left|\Delta x_i\right|^r\\
      \left|\Delta x_i\right| = C\left|\Delta x_{i-1}\right|^r
    \end{cases*} \Rightarrow \dfrac{\left|\Delta x_{i+1}\right|}{\left|\Delta x_i\right|} = \left(\dfrac{\left|\Delta x_i\right|}{\left|\Delta x_{i-1}\right|}\right)^r\\
    \Lrarr r = \dfrac{\ln\left|\dfrac{\Delta x_{i+1}}{\Delta x_i}\right|}{\ln\left|\dfrac{\Delta x_i}{\Delta x_{i-1}}\right|}
  \end{gathered}
\end{equation*}\par
\noindent Givet att vi vet värden på $\left|\Delta x_i\right|$

\subsection{Sekantmetoden}\hfill\\
\par\bigskip
\noindent Den är väldigt lik Newtons-Raphson, i någon mening är derivata bara en väldigt väldigt fin sekant. Som man hör på namnet så tar vi alltså en sekant istället för en tangent. Därför har den i någon mening samma form som Newton-Raphson:


\begin{equation*}
  \begin{gathered}
    x_{i+1} = x_i+\Delta x_i\\
    \text{I Newton-Raphson har vi } \Delta x_i = -\dfrac{f(x)}{f^{\prime}(x)}\\
    \text{I Sekantmetoden har vi en uppskattning på sekantens lutning:}\\
    f^{\prime}(x_i)\approx \dfrac{f(x_i)-f(x_{i-1})}{x_i-x_{i-1}}\\
    \text{Insättning i Newton-Raphson ger:}\\
    \Delta x_i = -f(x_i)\dfrac{x_i-x_{i-1}}{f(x_i)-f(x_{i-1})}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Vi behöver 2 startgissningar, men vi behöver inte räkna någon derivata. Konvergensordningen är något lägre än Newton-Raphson, men fortfarande superlinjärt ($r=\dfrac{1+\sqrt{5}}{2}$) (gyllene snittet!!)
\par\bigskip
\noindent En vanlig tillämpning av Newton-Raphson är inom miniräknare för att räkna ut roten ur ett tal.
\par\bigskip
\noindent Exempel: Bestäm $\sqrt{n}$ för givet $n$.\par
\noindent Vi vill lösa $x=\sqrt{n}$ vilket är samma som att lösa $x^2=n\Lrarr x^2-n=0$. Nu kan vi använda Newton-Raphson:

\begin{equation*}
  \begin{gathered}
    f^{\prime}(x) = 2x\\
    x_{i+1}=x_i-\dfrac{f(x_i)}{f^{\prime}(x_i)}=x_i-\dfrac{x_i^2-n}{2x_i}\\
    \text{För } \sqrt{423}\text{ med 4 korrekta decimaler (dvs} \left|x_*-\tilde{x}\right|<0.5\cdot10^{-4}\text{). En rimlig gissning är $x_0 = \sqrt{400} = 20$:}\\
    x_1 = 20-\dfrac{20^2-423}{2\cdot20}=20+0.575=20.575\\
    \text{Notera att 0.575 är större än toleransen, vi fortsätter:}\\
  x_2 = x_1-\dfrac{x_1^2-n}{2x_1} = 20.575-\dfrac{(20.575)^2-423}{2\cdot20.575}\approx \underbrace{20,575-0.0080346}_{\text{$\left|\Delta x_1\right| > 0.5\cdot10^{-4}$, fortsätt}}\approx 20.56696537\\
  x_3 = x_2 -\dfrac{x_2^2-n}{2x_2} = 20.56696537-\underbrace{1.57\cdot10^{-6}}_{\text{$=\left|\Delta x_2\right|<0.5\cdot10^{-4}$ Ok!}}
  \end{gathered}
\end{equation*}
\par\bigskip
\noindent Övning! Bestäm $p$:te roten ur $n$, dvs $x = n^{1/p}$
\par\bigskip
\noindent Exempel: Startgissning + SciPy\par
\noindent Givet:

\begin{equation*}
  \begin{gathered}
    x^{12}+x=0.1\Lrarr f(x)=x^{12}+x-0.1=0
  \end{gathered}
\end{equation*}\par
\noindent Nu är vi ute efter en startgissning för att köra Newton-Raphson på den. Genom att titta på ekvationen ser vi att $x$ är relativt liten. Vi vet även att $x^{12}<x$ viket ger oss:

\begin{equation*}
  \begin{gathered}
    f(x)\approx x-0.1=0\Lrarr x = 0.1\\
    \text{Kontroll av antagande: } x = 0.1\Rightarrow x^{12}<x\text{Ok!}
  \end{gathered}
\end{equation*}
\par
\noindent Vi skriver en SciPy lösning till detta:

\begin{verbatim}
from scipy import optimize

fun = lambda x: x**12+x-0.1
fp = lambda x: 12*x**11+1

#stoppar man inte in fprime så kör den sekant, busigt!
x = optimize.newton(fun, 0.1, fprime = fp) 
\end{verbatim}
\par\bigskip
\noindent Ytterliggare exempel: Lös $100e^x-x^2=10^{12}$\par
\noindent Skriver vi om den till $f(x) = 100e^x-x^2-10^{12}=0$ ser vi att $x$ måste vara stor för att kompensera mot $10^{12}$, gissar vi att $x$ är stor så antar vi helt enkelt att:

\begin{equation*}
  \begin{gathered}
    100e^x>x^2\Rightarrow f(x)\approx 100e^x-10^{12}=0\Rightarrow e^x = 10^{10}\Rightarrow x = 10\log(10)\approx 23
  \end{gathered}
\end{equation*}\par
\noindent Vi kontrollerar antagantet: $100e^{23}>23^2 \text{Ok!}$






